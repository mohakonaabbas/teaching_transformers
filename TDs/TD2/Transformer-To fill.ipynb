{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eea26f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d089dfc3-ed80-45aa-90fc-f35bfe26e8de",
   "metadata": {},
   "source": [
    "# Votre premier transformer\n",
    "\n",
    "Nous allons implementer de zero en Pytorch chaque composant d'un transformer \n",
    "\n",
    "\n",
    "* Le transformer que nous allons implementer est le transformer original presenté dans le papier \"Attention is All you Need.\" Nous allons implementer au fur et à chaque composant. Le td est en 2 parties : Une partie sur la création d'un transformer, puis une partie sur l'entrainement qui permettra d'entrainer des transformers sur des données textuelles.\n",
    "\n",
    "![Transformers Architecture](https://i0.wp.com/i.postimg.cc/Bn7QmpQS/1-43lg-CTy-M5c-TTABj-C2-VEHd-A.png?resize=579%2C800&ssl=1)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d68d898d",
   "metadata": {},
   "source": [
    "## Inputs Embeddings\n",
    "\n",
    "https://pytorch.org/docs/stable/generated/torch.nn.Embedding.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc1925d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class InputEmbedding(nn.Module):\n",
    "    def __init__(self, num_embeddings:int,\n",
    "                embedding_dim:int):\n",
    "        \n",
    "        super(InputEmbedding,self).__init__()\n",
    "       # TO DO\n",
    "        \n",
    "    def forward(self,x):\n",
    "        return NotImplementedError\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "324e7985",
   "metadata": {},
   "source": [
    "## Positional Encoding\n",
    "\n",
    "$$P(k,2i)= sin(\\frac{k}{n^{2i/d}})$$\n",
    "$$P(k,2i+1)= sin(\\frac{k}{n^{(2i+1)/d}})$$\n",
    "* k : longeur du contexte\n",
    "* d : taille de l'embedding\n",
    "* n : 10000 , hyperparamètre\n",
    "* $i \\in [0, d/2]$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6ef2028",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEmbedding(nn.Module):\n",
    "    def __init__(self, embedding_dim : int,\n",
    "                context_length : int,\n",
    "                user_defined_scalar : int = 10000):\n",
    "        \n",
    "        super(PositionalEmbedding,self).__init__()\n",
    "        \n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.context_length = context_length\n",
    "\n",
    "        self.n = user_defined_scalar\n",
    "        \n",
    "        #Intialise positional embedding\n",
    "        self.positional_embeddings = torch.zeros((self.context_length, self.embedding_dim))\n",
    "        \n",
    "       #TO DO\n",
    "        \n",
    "    def forward(self,x):\n",
    "        return self.positional_embeddings[x,:]\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ce3f529",
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_positional_embedding(positional_embedding):\n",
    "    plt.imshow(positional_embedding.positional_embeddings,cmap='viridis')\n",
    "    plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96ccea5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "tic = time.time()\n",
    "pe=PositionalEmbedding(250,500)\n",
    "toc = time.time()\n",
    "display_positional_embedding(pe)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b81e3bd5",
   "metadata": {},
   "source": [
    "## Multihead Attention\n",
    "![Multi- Head Attention](https://i0.wp.com/i.postimg.cc/G23vwqn4/Screenshot-from-2019-06-17-22-47-53.webp?w=1230&ssl=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd129974",
   "metadata": {},
   "source": [
    "### Define a simple attention head\n",
    "* https://pytorch.org/docs/stable/generated/torch.nn.Linear.html\n",
    "* https://pytorch.org/docs/stable/generated/torch.matmul.html\n",
    "* https://pytorch.org/docs/stable/generated/torch.nn.functional.softmax.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "029e4a8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfAttention(nn.Module):\n",
    "    def __init__(self, embedding_dim: int,\n",
    "                 hidden_dimension:int,\n",
    "                 out_dim : int = 0  ):\n",
    "        super(SelfAttention,self).__init__()\n",
    "        #TODO\n",
    "\n",
    "    def forward(self,x):\n",
    "        B, T, C = x.size() # batch size, sequence length, embedding dimensionality (embedding_dim)\n",
    "        \n",
    "        # TO DO\n",
    "\n",
    "        return out, attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e19fda76",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.rand((11,10,784))\n",
    "sa_module = SelfAttention(784,10)\n",
    "out, attention = sa_module(x)\n",
    "out.shape , attention.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8299b39",
   "metadata": {},
   "source": [
    "### Masked MultiHead Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2cd30e5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MaskedSelfAttention(SelfAttention):\n",
    "    def __init__(self, embedding_dim: int, hidden_dimension:int,out_dim : int = 0  ):\n",
    "        super(MaskedSelfAttention,self).__init__(embedding_dim,hidden_dimension,out_dim)\n",
    "        #TO DO\n",
    "\n",
    "        \n",
    "    def forward(self,x):\n",
    "       # TO DO\n",
    "\n",
    "        return out, attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41d14a8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.rand((11,10,784))\n",
    "sa_module = MaskedSelfAttention(784,10)\n",
    "out, attention = sa_module(x)\n",
    "out.shape , attention.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2851ddac",
   "metadata": {},
   "source": [
    "### Define multihead attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76901ada",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, embedding_dim : int , n_head :int, hidden_dimension : int ):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        #TO DO\n",
    "        \n",
    "    def forward(self,x):\n",
    "        B, T, C = x.size() # batch size, sequence length, embedding dimensionality (embedding_dim)\n",
    "        \n",
    "        #TO DO\n",
    "        return out , out_attention\n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b30eb99e",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = 784\n",
    "n_heads = 4\n",
    "hidden_dimension = 784\n",
    "context_lenght = 10\n",
    "bs = 11\n",
    "mha = MultiHeadAttention(embedding_dim,n_heads,hidden_dimension)\n",
    "x = torch.rand((bs,context_lenght,embedding_dim))\n",
    "out, attention = mha(x)\n",
    "out.shape , attention.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "038263f5",
   "metadata": {},
   "source": [
    "## Masked Multihead Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec325222",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MaskedMultiHeadAttention(MultiHeadAttention):\n",
    "    #TO DO\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9793e827",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = 784\n",
    "n_heads = 16\n",
    "hidden_dimension = 784\n",
    "context_lenght = 10\n",
    "bs = 11\n",
    "mha = MaskedMultiHeadAttention(embedding_dim,n_heads,hidden_dimension)\n",
    "x = torch.rand((bs,context_lenght,embedding_dim))\n",
    "out, attention = mha(x)\n",
    "out.shape , attention.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb835d57",
   "metadata": {},
   "source": [
    "## Cross MultiHead Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2706c15",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CrossAttention(nn.Module):\n",
    "    def __init__(self, embedding_dim: int, hidden_dimension:int, out_dim : int = 0  ):\n",
    "        super(CrossAttention,self).__init__()\n",
    "        # TO DO \n",
    "    def forward(self, x , context):\n",
    "        # TO DO\n",
    "\n",
    "        return out, attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31820727",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CrossMultiHeadAttention(MultiHeadAttention):\n",
    "    def __init__(self, embedding_dim : int , n_head :int, hidden_dimension : int ):\n",
    "        super(CrossMultiHeadAttention, self).__init__(embedding_dim,n_head,hidden_dimension)\n",
    "       # TO DO\n",
    "    def forward(self,x,context):\n",
    "# TO DO\n",
    "        return out , out_attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80f53a8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = 784\n",
    "n_heads = 16\n",
    "hidden_dimension = 784\n",
    "context_lenght = 10\n",
    "bs = 11\n",
    "cha = CrossMultiHeadAttention(embedding_dim,n_heads,hidden_dimension)\n",
    "x = torch.rand((bs,context_lenght,embedding_dim))\n",
    "context = torch.rand((bs,context_lenght//2,embedding_dim))\n",
    "out, attention = cha(x,context)\n",
    "out.shape , attention.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "297de72a",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## Feed Forward Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57637845",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, embedding_dim, hidden_dim):\n",
    "        super(FeedForward, self).__init__()\n",
    "#TO DO\n",
    "\n",
    "    def forward(self, x):\n",
    "        return # TO DO\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "694308a8",
   "metadata": {},
   "source": [
    "## Block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "228cd810",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderBlock(nn.Module):\n",
    "    def __init__(self, # TO DO):\n",
    "        super(EncoderBlock, self).__init__()\n",
    "        \n",
    "        # TO DO\n",
    "        \n",
    "    def forward(self,x):\n",
    "        # TO DO\n",
    "        return out, att\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45c4e910",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = 784\n",
    "n_heads = 16\n",
    "hidden_dimension = 784\n",
    "context_lenght = 10\n",
    "bs = 11\n",
    "n_blocks  = 5\n",
    "vocab_size = 3000\n",
    "context_size = 40\n",
    "enc_bloc = EncoderBlock(embedding_dim,hidden_dimension,hidden_dimension,n_heads)\n",
    "x = torch.rand((bs,context_lenght,embedding_dim))\n",
    "enc , attn = enc_bloc(x)\n",
    "enc.shape , attn.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6ec1a0c",
   "metadata": {},
   "source": [
    "## Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "189f91ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerEncoder(nn.Module):\n",
    "    def __init__(self, embedding_dim: int,\n",
    "                 qk_hidden_dimension:int,\n",
    "                 feedforward_hidden_dimension : int,\n",
    "                 n_heads : int ,\n",
    "                n_blocks : int,\n",
    "                vocab_size : int,\n",
    "                context_size : int):\n",
    "        super(TransformerEncoder, self).__init__()\n",
    "        \n",
    "       # TO DO\n",
    "    def forward(self,x):\n",
    "       #TO DO\n",
    "        return x , attns\n",
    "                                    \n",
    "                                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e762e1b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = TransformerEncoder(embedding_dim,hidden_dimension,hidden_dimension,n_heads,n_blocks,vocab_size, context_size)\n",
    "x = torch.randint(20,(10,10))\n",
    "encoding , attns = encoder(x)\n",
    "encoding.shape , len(attns), attns[0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c706354e",
   "metadata": {},
   "source": [
    "## Decoder\n",
    "\n",
    "# A vous de jouer\n",
    "\n",
    "En reutilisant au maximum les composants au dessus, implementer le decodeur et le transformeur final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd422958",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f2d95803",
   "metadata": {},
   "source": [
    "## Transformer\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d89c194",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "phd",
   "language": "python",
   "name": "phd"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
