{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d089dfc3-ed80-45aa-90fc-f35bfe26e8de",
   "metadata": {},
   "source": [
    "Let define a Tokenizer class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "594850b9-c064-46e7-97b4-61ce780eb980",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "c000c140-8379-4dd3-9f37-64844a6e23ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"pg55738.txt\",\"r\") as f:\n",
    "    entry_text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "d009f537-35c5-4c89-8fe3-fb277c5d779e",
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern = r\"\"\"'(?i:[sdmt]|ll|ve|re)|[^\\r\\n\\p{L}\\p{N}]?+\\p{L}+|\\p{N}{1,3}| ?[^\\s\\p{L}\\p{N}]++[\\r\\n]*|\\s*[\\r\\n]|\\s+(?!\\S)|\\s+\"\"\"\n",
    "text_chunks = re.findall(pattern, entry_text)\n",
    "ids = [list(ch) for ch in text_chunks]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07ccc23c-62b2-4b9c-85d4-898ebfeeeec2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "7d445bba-f48e-4c3b-813b-77e2069934f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def define_initial_vocab(input_text):\n",
    "    init_vocab = set(input_text)\n",
    "    return sorted(list(init_vocab))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "0fb26efd-f607-488a-8a63-13fd2e98e00d",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = list(entry_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "5cdc7906-d4ef-4c03-be9d-3f1c729997e8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def count_pair_occurences(input_text):\n",
    "    count ={}\n",
    "\n",
    "    for i in range(len(input_text)-1):\n",
    "        pair =(input_text[i],input_text[i+1])\n",
    "        if pair in count:\n",
    "            count[pair]+=1\n",
    "        else :\n",
    "            count[pair]=1\n",
    "    return count\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "55201f92-90d9-45e5-88c4-56a62e5e0ba7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_max_occurence(occurence_count):\n",
    "    old_val=0\n",
    "    for key,val in occurence_count.items():\n",
    "        if val>old_val:\n",
    "            max_key = key\n",
    "            old_val = val\n",
    "    return max_key,old_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "eb365827-3e00-4d3a-8eb2-96e864e6d31c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge(text, pair):\n",
    "    \"\"\"Splits a token on a given substring.\n",
    "\n",
    "    Args:\n",
    "        token: The token to split.\n",
    "        substring: The substring to split on.\n",
    "\n",
    "    Returns:\n",
    "        A list of the split tokens.\n",
    "    \"\"\"\n",
    "    newtext = []\n",
    "    i = 0\n",
    "    while i < len(text):\n",
    "        # if not at the very last position AND the pair matches, replace it\n",
    "        if text[i] == pair[0] and i < len(text) - 1 and text[i+1] == pair[1]:\n",
    "            newtext.append(\"\".join(pair))\n",
    "            i += 2\n",
    "        else:\n",
    "            newtext.append(text[i])\n",
    "            i += 1\n",
    "    return newtext\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "e1879872-cccb-46f0-b10b-5abda60023ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_iterations = 1000\n",
    "text_=text\n",
    "vocab = define_initial_vocab(entry_text)\n",
    "merges = {}\n",
    "init_vocab_size = len(vocab)\n",
    "for _ in range(n_iterations):\n",
    "    occurence_count = count_pair_occurences(text_)\n",
    "    key,val = find_max_occurence(occurence_count)\n",
    "    if val ==1 :\n",
    "        break\n",
    "    vocab.append(\"\".join(key))\n",
    "    merges[key]=\"\".join(key)\n",
    "\n",
    "    text_ = merge(text_, key)\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "74b02a9a-f520-42d6-b862-0d3aebd26396",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['\\n',\n",
       " ' ',\n",
       " '(',\n",
       " ')',\n",
       " '*',\n",
       " ',',\n",
       " '-',\n",
       " '.',\n",
       " 'A',\n",
       " 'B',\n",
       " 'C',\n",
       " 'D',\n",
       " 'E',\n",
       " 'F',\n",
       " 'G',\n",
       " 'H',\n",
       " 'I',\n",
       " 'L',\n",
       " 'O',\n",
       " 'P',\n",
       " 'S',\n",
       " 'T',\n",
       " 'U',\n",
       " 'W',\n",
       " 'a',\n",
       " 'b',\n",
       " 'c',\n",
       " 'd',\n",
       " 'e',\n",
       " 'f',\n",
       " 'g',\n",
       " 'h',\n",
       " 'i',\n",
       " 'j',\n",
       " 'k',\n",
       " 'l',\n",
       " 'm',\n",
       " 'n',\n",
       " 'o',\n",
       " 'p',\n",
       " 'q',\n",
       " 'r',\n",
       " 's',\n",
       " 't',\n",
       " 'u',\n",
       " 'v',\n",
       " 'w',\n",
       " 'x',\n",
       " 'y',\n",
       " 'z',\n",
       " 'e ',\n",
       " 's ',\n",
       " 'an',\n",
       " 'en',\n",
       " 'or',\n",
       " 'd ',\n",
       " 'th',\n",
       " 'in',\n",
       " ', ',\n",
       " 'er',\n",
       " 'y ',\n",
       " 'at',\n",
       " 'on',\n",
       " 'of',\n",
       " 'ol',\n",
       " 'and ',\n",
       " '. ',\n",
       " 'of ',\n",
       " 'ic',\n",
       " 'ar',\n",
       " 'al',\n",
       " 're',\n",
       " 'it',\n",
       " 'ur',\n",
       " ' c',\n",
       " ' p',\n",
       " 'st',\n",
       " 'to',\n",
       " 'ed ',\n",
       " 'ing',\n",
       " 'the ',\n",
       " 'is ',\n",
       " ' a',\n",
       " 'or ',\n",
       " 'es',\n",
       " 'ion',\n",
       " 'ent',\n",
       " 'ing ',\n",
       " 'ec',\n",
       " ' s',\n",
       " '\\n\\n',\n",
       " 'ir',\n",
       " 'pl',\n",
       " 'Th',\n",
       " 'ss',\n",
       " 'ro',\n",
       " 'iv',\n",
       " 'un',\n",
       " 'es ',\n",
       " 'are ',\n",
       " 'ci',\n",
       " '.\\n\\n',\n",
       " 'an ',\n",
       " 'our',\n",
       " 'AI',\n",
       " 'ut',\n",
       " 's, ',\n",
       " 'ex',\n",
       " 'el',\n",
       " ' is ',\n",
       " 'hen',\n",
       " 'that',\n",
       " 'al ',\n",
       " 'em',\n",
       " 'to ',\n",
       " 'if',\n",
       " 'ichen',\n",
       " 'ch',\n",
       " 'sy',\n",
       " 'wi',\n",
       " '. Th',\n",
       " 'om',\n",
       " ', and ',\n",
       " 'e c',\n",
       " 'olor',\n",
       " 'in ',\n",
       " 'et',\n",
       " 'ac',\n",
       " 'ev',\n",
       " 'e of ',\n",
       " 'olo',\n",
       " 'olog',\n",
       " 'ce',\n",
       " 'our ',\n",
       " 'oci',\n",
       " 'lu',\n",
       " 'ten',\n",
       " 'is',\n",
       " 'wor',\n",
       " ' and ',\n",
       " 'ha',\n",
       " 'ab',\n",
       " 's of ',\n",
       " 'il',\n",
       " 'im',\n",
       " 'ant',\n",
       " ' ass',\n",
       " 'us',\n",
       " 'that ',\n",
       " 'y of ',\n",
       " 'olor ',\n",
       " 'hav',\n",
       " 'ive ',\n",
       " 'e a',\n",
       " 'ichens ',\n",
       " 'the',\n",
       " 'und',\n",
       " 'li',\n",
       " 'id',\n",
       " 'ce ',\n",
       " 'e, ',\n",
       " 'ated ',\n",
       " 'with',\n",
       " '**',\n",
       " 'en ',\n",
       " 'often',\n",
       " 'e p',\n",
       " 'ot',\n",
       " 'ation',\n",
       " 'ob',\n",
       " 'ect',\n",
       " 'ir ',\n",
       " 'ly ',\n",
       " 'es. ',\n",
       " '. B',\n",
       " 'ence ',\n",
       " 'ific',\n",
       " 'um',\n",
       " 'mor',\n",
       " 'sych',\n",
       " 'sycholog',\n",
       " ' associ',\n",
       " 'ated with',\n",
       " 'ism',\n",
       " 'as',\n",
       " 'org',\n",
       " 'organ',\n",
       " 'ru',\n",
       " 'stem',\n",
       " 'worl',\n",
       " 'sh',\n",
       " 'a ',\n",
       " 'mo',\n",
       " 'lichens ',\n",
       " ' con',\n",
       " 'dit',\n",
       " 'dition',\n",
       " 's c',\n",
       " '. The',\n",
       " 'qu',\n",
       " ' pro',\n",
       " 'imp',\n",
       " 'ity',\n",
       " '. By ',\n",
       " 'ing the ',\n",
       " 'hum',\n",
       " 'For ',\n",
       " 'For ex',\n",
       " 'For exa',\n",
       " 'For exam',\n",
       " 'For exampl',\n",
       " 'For example, ',\n",
       " 'ap',\n",
       " 'ial ',\n",
       " ' color',\n",
       " 'often associ',\n",
       " 'often associated with',\n",
       " 'The ',\n",
       " 'lo',\n",
       " 'as ',\n",
       " 'on ',\n",
       " 'organism',\n",
       " ' pl',\n",
       " 'rol',\n",
       " 'stems ',\n",
       " 'world',\n",
       " ' re',\n",
       " 'bet',\n",
       " 'we',\n",
       " 'have ',\n",
       " 'vir',\n",
       " 'ment',\n",
       " 'abl',\n",
       " 'ther',\n",
       " 'ert',\n",
       " 'their ',\n",
       " 'du',\n",
       " 'als',\n",
       " 'also',\n",
       " 'impor',\n",
       " 'import',\n",
       " 'bec',\n",
       " 'becom',\n",
       " 'be',\n",
       " 'used ',\n",
       " ' per',\n",
       " 'ain',\n",
       " '. By und',\n",
       " '. By under',\n",
       " '. By underst',\n",
       " '. By understan',\n",
       " '. By understand',\n",
       " 'we c',\n",
       " 'we can ',\n",
       " 'its ',\n",
       " 'AI is ',\n",
       " 'it is ',\n",
       " 'human',\n",
       " '. A',\n",
       " ' can ',\n",
       " 'Color',\n",
       " 'inf',\n",
       " 'influ',\n",
       " 'influence ',\n",
       " 'eo',\n",
       " 'eopl',\n",
       " 'eople ',\n",
       " 'more ',\n",
       " '**The ',\n",
       " '**\\n\\n',\n",
       " 'ed and ',\n",
       " 'iss',\n",
       " 'color ',\n",
       " 'tre',\n",
       " 'fas',\n",
       " 'fasc',\n",
       " 'fascin',\n",
       " 'fascinat',\n",
       " 'fascinating ',\n",
       " ' pla',\n",
       " 'y c',\n",
       " 'bi',\n",
       " 'lation',\n",
       " 'lationsh',\n",
       " 'lationshi',\n",
       " 'lationship',\n",
       " 'betwe',\n",
       " 'act',\n",
       " 'e of the ',\n",
       " 'e of the mo',\n",
       " 'e of the most',\n",
       " 'envir',\n",
       " 'environ',\n",
       " 'environment',\n",
       " 'mar',\n",
       " 'mark',\n",
       " 'sp',\n",
       " 'y to',\n",
       " ' sur',\n",
       " 'wh',\n",
       " 'e f',\n",
       " 'oun',\n",
       " 'ct',\n",
       " ', and ev',\n",
       " ' par',\n",
       " ' part',\n",
       " 'ner',\n",
       " 'ure ',\n",
       " 'ri',\n",
       " 'thes',\n",
       " ', p',\n",
       " ' com',\n",
       " 'important',\n",
       " 'important ',\n",
       " 'dic',\n",
       " 'qual',\n",
       " 'y are ',\n",
       " 'ig',\n",
       " 'ens',\n",
       " 'oll',\n",
       " 'ollut',\n",
       " 'su',\n",
       " ' d',\n",
       " 'pre',\n",
       " 'become ',\n",
       " 'in a',\n",
       " 'ist',\n",
       " 'ddition',\n",
       " 'for ',\n",
       " 'dy',\n",
       " 'plic',\n",
       " 'plication',\n",
       " 'utur',\n",
       " 'uture of ',\n",
       " 'for',\n",
       " 'liv',\n",
       " 's AI',\n",
       " 'ential ',\n",
       " 'job',\n",
       " ' sy',\n",
       " ' systems ',\n",
       " 'ement',\n",
       " 'ow',\n",
       " ' can also',\n",
       " 'dev',\n",
       " 'devel',\n",
       " 'develo',\n",
       " 'develop',\n",
       " '. By understanding the ',\n",
       " 'ef',\n",
       " 'sychology of ',\n",
       " 'influence our ',\n",
       " ' perce',\n",
       " 'pt',\n",
       " 'iff',\n",
       " 'iffer',\n",
       " 'ifferent',\n",
       " '. For example, ',\n",
       " 'red ',\n",
       " 'is often associated with',\n",
       " 'reen',\n",
       " 'eople are ',\n",
       " 'eople are more ',\n",
       " 'eople are more li',\n",
       " 'eople are more lik',\n",
       " 'eople are more likel',\n",
       " ' colors ',\n",
       " 'ea',\n",
       " 'Lichen',\n",
       " ' o',\n",
       " 'ok',\n",
       " 'ed as ',\n",
       " 'es of ',\n",
       " 'roc',\n",
       " 'rock',\n",
       " 's and ',\n",
       " 's that',\n",
       " 'ruci',\n",
       " 'rucial ',\n",
       " 'es in ',\n",
       " 'eco',\n",
       " 'ecosy',\n",
       " 'ung',\n",
       " 'alg',\n",
       " 'or c',\n",
       " 'or cy',\n",
       " 'or cyan',\n",
       " 'or cyanob',\n",
       " 'or cyanobact',\n",
       " 'or cyanobacter',\n",
       " 'or cyanobacteri',\n",
       " 'in s',\n",
       " ' ex',\n",
       " 'trem',\n",
       " 'treme ',\n",
       " 'treme environment',\n",
       " '.\\n\\nO',\n",
       " '.\\n\\nOn',\n",
       " '.\\n\\nOne of the most',\n",
       " 'markabl',\n",
       " 'ects of ',\n",
       " 'ir a',\n",
       " 'ir ab',\n",
       " 'ir abil',\n",
       " 'ir abilit',\n",
       " 'ir ability to',\n",
       " 'ir ability to sur',\n",
       " 'ir ability to surv',\n",
       " 'ir ability to survive ',\n",
       " 'ir ability to survive in ',\n",
       " 'har',\n",
       " 'ew',\n",
       " 'ther ',\n",
       " 's can',\n",
       " 'an b',\n",
       " 'ound ',\n",
       " ' t',\n",
       " 'ra',\n",
       " ', and even ',\n",
       " 'ys',\n",
       " 'al part',\n",
       " 'al partner',\n",
       " 'es a',\n",
       " ' prot',\n",
       " ' protect',\n",
       " 'er and ',\n",
       " 'rom',\n",
       " ' the ',\n",
       " 'rodu',\n",
       " ' comp',\n",
       " ' compo',\n",
       " ' compound',\n",
       " 'th ',\n",
       " 'organisms',\n",
       " '.\\n\\nL',\n",
       " '.\\n\\nLichens ',\n",
       " '.\\n\\nLichens are ',\n",
       " 'indic',\n",
       " 'indicat',\n",
       " 'indicator',\n",
       " 'indicators of ',\n",
       " 'indicators of a',\n",
       " 'indicators of air ',\n",
       " 'indicators of air qual',\n",
       " 'indicators of air quality',\n",
       " 'itive ',\n",
       " 'ollutant',\n",
       " 'such',\n",
       " 'such a',\n",
       " 'such as ',\n",
       " 'ox',\n",
       " 'oxid',\n",
       " 's are ',\n",
       " 'pres',\n",
       " 'the a',\n",
       " 'the air',\n",
       " ', lichens ',\n",
       " 'ress',\n",
       " 'di',\n",
       " 'mon',\n",
       " 'ul',\n",
       " 'ar ',\n",
       " 'In',\n",
       " ' to ',\n",
       " ' sig',\n",
       " ' sign',\n",
       " ' signific',\n",
       " 'ance',\n",
       " 'd, ',\n",
       " 'es. For example, ',\n",
       " 'ont',\n",
       " 's that ',\n",
       " 'nat',\n",
       " 'tru',\n",
       " 'div',\n",
       " 'diver',\n",
       " 'divers',\n",
       " 'plications ',\n",
       " 'ma',\n",
       " 'and ap',\n",
       " 'y and ',\n",
       " 'anc',\n",
       " 'ance of ',\n",
       " 's, we can ',\n",
       " '.\\n\\n**The ',\n",
       " 'Ar',\n",
       " 'Art',\n",
       " 'Artific',\n",
       " 'Artificial ',\n",
       " 'tel',\n",
       " 'telli',\n",
       " 'tellig',\n",
       " 'form',\n",
       " 'forming ',\n",
       " 'our world',\n",
       " 'riv',\n",
       " 'tu',\n",
       " 'inc',\n",
       " 'lives. ',\n",
       " 'ues ',\n",
       " ', it is ',\n",
       " 'to con',\n",
       " 'to cons',\n",
       " 'to consid',\n",
       " 'to consider',\n",
       " 'pot',\n",
       " 'potential ',\n",
       " 'soci',\n",
       " 'societ',\n",
       " 'future of ',\n",
       " 'ut ',\n",
       " '. As AI',\n",
       " '. As AI systems ',\n",
       " '. As AI systems become ',\n",
       " '. As AI systems become mor',\n",
       " 'is a ',\n",
       " 'ris',\n",
       " 'risk',\n",
       " 'that AI',\n",
       " 'reat',\n",
       " 'wil',\n",
       " 'will',\n",
       " 'e a s',\n",
       " 'work',\n",
       " 'ical ',\n",
       " 's of AI',\n",
       " 'it is c',\n",
       " 'ensure ',\n",
       " 'ensure that ',\n",
       " 'they are ',\n",
       " 'developed and ',\n",
       " 'developed and used ',\n",
       " 'developed and used in a',\n",
       " 'bl',\n",
       " 'y, and ',\n",
       " 'abil',\n",
       " 'ability',\n",
       " 'ertain',\n",
       " 'role ',\n",
       " 'ben',\n",
       " 'benef',\n",
       " 'benefits ',\n",
       " ', we can ',\n",
       " 'wa',\n",
       " 'y that ',\n",
       " 'emot',\n",
       " 'emotion',\n",
       " 'behav',\n",
       " 'behavi',\n",
       " 'behavior',\n",
       " 'our perce',\n",
       " 'our percept',\n",
       " 'our perception',\n",
       " 'our perceptions of ',\n",
       " 'our perceptions of the ',\n",
       " 'e psychology of ',\n",
       " 'e psychology of color ',\n",
       " 'fascinating f',\n",
       " 'fascinating fi',\n",
       " 'fascinating fiel',\n",
       " 'fascinating field ',\n",
       " 'fascinating field of ',\n",
       " 'fascinating field of st',\n",
       " 'fascinating field of stu',\n",
       " 'relationship',\n",
       " 'relationship ',\n",
       " 'relationship betwe',\n",
       " 'relationship between',\n",
       " 'relationship between c',\n",
       " 'relationship between color ',\n",
       " 'relationship between color and ',\n",
       " 'relationship between color and human',\n",
       " 'relationship between color and human p',\n",
       " 'relationship between color and human psycholog',\n",
       " 'relationship between color and human psychology',\n",
       " 'ifferent color',\n",
       " 's. For example, ',\n",
       " 's. For example, red ',\n",
       " 's. For example, red is often associated with',\n",
       " 'ass',\n",
       " 'cit',\n",
       " 'citement',\n",
       " 'reen is ',\n",
       " 'reen is often associated with',\n",
       " 'use c',\n",
       " 'use color ',\n",
       " 'use color to ',\n",
       " 'use color to influence our ',\n",
       " 'pec',\n",
       " 'pecific',\n",
       " 'pecific ',\n",
       " '.\\n\\nColor',\n",
       " '.\\n\\nColor can also',\n",
       " 'eople are more likely ',\n",
       " 'eople are more likely to ',\n",
       " 'eople are more likely to b',\n",
       " 'ack',\n",
       " '. Addition',\n",
       " '. Additional',\n",
       " '. Additionall',\n",
       " '. Additionally',\n",
       " '. Additionally, p',\n",
       " 'blu',\n",
       " ' colors (',\n",
       " 'ang']"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "2b6e1e25-789f-4d99-bdc9-a503244b68cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_pair(query_pairs,merges_pairs):\n",
    "    query_result = []\n",
    "    for query in query_pairs.keys():\n",
    "        if query in merges_pairs:\n",
    "            query_result.append(1)\n",
    "        else :query_result.append(0)\n",
    "    if 1 in query_result :\n",
    "        idx = query_result.index(1)\n",
    "\n",
    "        return list(query_pairs.keys())[idx]\n",
    "    return None\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "825b88bf-f7b6-4a77-b12a-e48002a4c1ee",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def decode(characters):\n",
    "    return NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "19d94c0f-8a8a-4350-8c7d-298247a8f432",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def encode(text,merges):\n",
    "\n",
    "        text_ = list(text) # list of integers in range 0..255\n",
    "        while len(text_) >= 2:\n",
    "            # find the pair with the lowest merge index\n",
    "            occurence_count = count_pair_occurences(text_)\n",
    "            \n",
    "            pair = search_pair(occurence_count,merges)\n",
    "            \n",
    "            # pair = find_max_occurence(occurence_count)\n",
    "            # subtle: if there are no more merges available, the key will\n",
    "            # result in an inf for every single pair, and the min will be\n",
    "            # just the first pair in the list, arbitrarily\n",
    "            # we can detect this terminating case by a membership check\n",
    "            if pair not in merges:\n",
    "                break # nothing else can be merged anymore\n",
    "            # otherwise let's merge the best pair (lowest merge index)\n",
    "            idx = occurence_count[pair]\n",
    "            text_ = merge(text_, pair)\n",
    "            # print(pair, text_,\"\\n\")\n",
    "        return text_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "02991192-c6ad-4209-b157-1992d28a11a4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test_data = entry_text[:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "bc858430-459b-42c5-97c8-0492db4b549b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['**',\n",
       " 'The ',\n",
       " 'U',\n",
       " 'n',\n",
       " 's',\n",
       " 'e',\n",
       " 'en ',\n",
       " 'B',\n",
       " 'e',\n",
       " 'a',\n",
       " 'ut',\n",
       " 'y of ',\n",
       " 'L',\n",
       " 'ichen',\n",
       " 's',\n",
       " '**',\n",
       " '\\n\\n',\n",
       " 'L',\n",
       " 'ichen',\n",
       " 's, ',\n",
       " 'of',\n",
       " 't',\n",
       " 'en ',\n",
       " 'o',\n",
       " 'v',\n",
       " 'er',\n",
       " 'lo',\n",
       " 'o',\n",
       " 'k',\n",
       " 'ed ',\n",
       " 'and ',\n",
       " 'd',\n",
       " 'ism',\n",
       " 'is',\n",
       " 's',\n",
       " 'ed ',\n",
       " 'as',\n",
       " ' ',\n",
       " 'm',\n",
       " 'er',\n",
       " 'e p',\n",
       " 'at',\n",
       " 'ch',\n",
       " 'es',\n",
       " ' ',\n",
       " 'of ',\n",
       " 'c',\n",
       " 'o',\n",
       " 'lo',\n",
       " 'r',\n",
       " ' ',\n",
       " 'on ',\n",
       " 'ro',\n",
       " 'c',\n",
       " 'k',\n",
       " 's ',\n",
       " 'and ',\n",
       " 't',\n",
       " 're',\n",
       " 'e',\n",
       " 's, ',\n",
       " 'are ',\n",
       " 'f',\n",
       " 'as',\n",
       " 'ci',\n",
       " 'n',\n",
       " 'at',\n",
       " 'ing ',\n",
       " 'organism',\n",
       " 's ',\n",
       " 'that ',\n",
       " 'pl',\n",
       " 'a',\n",
       " 'y ',\n",
       " 'c',\n",
       " 'ru',\n",
       " 'ci',\n",
       " 'al ',\n",
       " 'ro',\n",
       " 'l',\n",
       " 'es',\n",
       " ' ',\n",
       " 'in ',\n",
       " 'ec',\n",
       " 'o',\n",
       " 'sy',\n",
       " 'stems ',\n",
       " 'world',\n",
       " 'wi',\n",
       " 'd',\n",
       " 'e',\n",
       " '. Th',\n",
       " 'es',\n",
       " 'e ',\n",
       " 'sy',\n",
       " 'm',\n",
       " 'b',\n",
       " 'i',\n",
       " 'ot',\n",
       " 'ic',\n",
       " ' re',\n",
       " 'l',\n",
       " 'ation',\n",
       " 'sh',\n",
       " 'i',\n",
       " 'p',\n",
       " 's ',\n",
       " 'be',\n",
       " 't',\n",
       " 'we',\n",
       " 'en ',\n",
       " 'f',\n",
       " 'un',\n",
       " 'g',\n",
       " 'i',\n",
       " ' and ',\n",
       " 'al',\n",
       " 'g',\n",
       " 'a',\n",
       " 'e ',\n",
       " 'or ',\n",
       " 'c',\n",
       " 'y',\n",
       " 'an',\n",
       " 'ob',\n",
       " 'ac',\n",
       " 't',\n",
       " 'er',\n",
       " 'i',\n",
       " 'a ',\n",
       " 'have ',\n",
       " 'ev',\n",
       " 'ol',\n",
       " 'v',\n",
       " 'ed ',\n",
       " 'to ',\n",
       " 'th',\n",
       " 'r',\n",
       " 'ive ',\n",
       " 'in ',\n",
       " 's',\n",
       " 'om',\n",
       " 'e of ',\n",
       " 'the ',\n",
       " 'mo',\n",
       " 'st',\n",
       " ' ',\n",
       " 'ex',\n",
       " 't',\n",
       " 're',\n",
       " 'm',\n",
       " 'e ',\n",
       " 'en',\n",
       " 'vir',\n",
       " 'on',\n",
       " 'ment',\n",
       " 's ',\n",
       " 'on ',\n",
       " 'E',\n",
       " 'ar',\n",
       " 'th',\n",
       " '.\\n\\n',\n",
       " 'O',\n",
       " 'n',\n",
       " 'e of ',\n",
       " 'the ',\n",
       " 'mo',\n",
       " 'st',\n",
       " ' re',\n",
       " 'm',\n",
       " 'ar',\n",
       " 'k',\n",
       " 'abl',\n",
       " 'e ',\n",
       " 'as',\n",
       " 'p',\n",
       " 'ect',\n",
       " 's of ',\n",
       " 'lichens ',\n",
       " 'is',\n",
       " ' ',\n",
       " 'their ',\n",
       " 'ab',\n",
       " 'il',\n",
       " 'it',\n",
       " 'y ',\n",
       " 'to ',\n",
       " 's',\n",
       " 'ur',\n",
       " 'v',\n",
       " 'ive ',\n",
       " 'in ',\n",
       " 'h',\n",
       " 'ar',\n",
       " 'sh',\n",
       " ' con',\n",
       " 'dition',\n",
       " 's ',\n",
       " 'w',\n",
       " 'h',\n",
       " 'er',\n",
       " 'e ',\n",
       " 'f',\n",
       " 'e',\n",
       " 'w',\n",
       " ' ',\n",
       " 'o',\n",
       " 'ther',\n",
       " ' ',\n",
       " 'organism',\n",
       " 's c',\n",
       " 'an',\n",
       " '. The',\n",
       " 'y ',\n",
       " 'c',\n",
       " 'an ',\n",
       " 'b',\n",
       " 'e ',\n",
       " 'f',\n",
       " 'o',\n",
       " 'un',\n",
       " 'd ',\n",
       " 'in ',\n",
       " 'd',\n",
       " 'es',\n",
       " 'ert',\n",
       " 's, ',\n",
       " 'ar',\n",
       " 'c',\n",
       " 't',\n",
       " 'ic',\n",
       " ' ',\n",
       " 't',\n",
       " 'und',\n",
       " 'r',\n",
       " 'a',\n",
       " ', and ',\n",
       " 'ev',\n",
       " 'en ',\n",
       " 'on ',\n",
       " 'b',\n",
       " 'are ',\n",
       " 'ro',\n",
       " 'c',\n",
       " 'k',\n",
       " ' s',\n",
       " 'ur',\n",
       " 'f',\n",
       " 'ac',\n",
       " 'es',\n",
       " '. The',\n",
       " 'ir ',\n",
       " 's',\n",
       " 'ec',\n",
       " 're',\n",
       " 't',\n",
       " ' ',\n",
       " 'li',\n",
       " 'es',\n",
       " ' ',\n",
       " 'in ',\n",
       " 'their ',\n",
       " 'un',\n",
       " 'i',\n",
       " 'qu',\n",
       " 'e p',\n",
       " 'h',\n",
       " 'y',\n",
       " 's',\n",
       " 'i',\n",
       " 'o',\n",
       " 'lo',\n",
       " 'g',\n",
       " 'y',\n",
       " '. ',\n",
       " 'The ',\n",
       " 'f',\n",
       " 'un',\n",
       " 'g',\n",
       " 'al ',\n",
       " 'p',\n",
       " 'ar',\n",
       " 't',\n",
       " 'n',\n",
       " 'er',\n",
       " ' pro',\n",
       " 'v',\n",
       " 'id',\n",
       " 'es',\n",
       " ' ',\n",
       " 'a ',\n",
       " 'p',\n",
       " 'ro',\n",
       " 't',\n",
       " 'ect',\n",
       " 'ive ',\n",
       " 'st',\n",
       " 'ru',\n",
       " 'c',\n",
       " 't',\n",
       " 'ur',\n",
       " 'e ',\n",
       " 'and ',\n",
       " 'ab',\n",
       " 's',\n",
       " 'or',\n",
       " 'b',\n",
       " 's ',\n",
       " 'w',\n",
       " 'at',\n",
       " 'er',\n",
       " ' and ',\n",
       " 'n',\n",
       " 'ut',\n",
       " 'r',\n",
       " 'i',\n",
       " 'ent',\n",
       " 's ',\n",
       " 'f',\n",
       " 'ro',\n",
       " 'm',\n",
       " ' ',\n",
       " 'the ',\n",
       " 'en',\n",
       " 'vir',\n",
       " 'on',\n",
       " 'ment',\n",
       " '. ',\n",
       " 'The ',\n",
       " 'al',\n",
       " 'g',\n",
       " 'al ',\n",
       " 'or ',\n",
       " 'c',\n",
       " 'y',\n",
       " 'an',\n",
       " 'ob',\n",
       " 'ac',\n",
       " 't',\n",
       " 'er',\n",
       " 'ial ',\n",
       " 'p',\n",
       " 'ar',\n",
       " 't',\n",
       " 'n',\n",
       " 'er',\n",
       " ' p',\n",
       " 'h',\n",
       " 'ot',\n",
       " 'o',\n",
       " 'sy',\n",
       " 'n',\n",
       " 'th',\n",
       " 'es',\n",
       " 'i',\n",
       " 'z',\n",
       " 'e',\n",
       " 's, ',\n",
       " 'p',\n",
       " 'ro',\n",
       " 'du',\n",
       " 'ci',\n",
       " 'n',\n",
       " 'g',\n",
       " ' ',\n",
       " 'organ',\n",
       " 'ic',\n",
       " ' c',\n",
       " 'om',\n",
       " 'p',\n",
       " 'o',\n",
       " 'und',\n",
       " 's ',\n",
       " 'that ',\n",
       " 's',\n",
       " 'u',\n",
       " 'st',\n",
       " 'a',\n",
       " 'in ',\n",
       " 'b',\n",
       " 'o',\n",
       " 'th',\n",
       " ' ',\n",
       " 'organism',\n",
       " 's',\n",
       " '.\\n\\n',\n",
       " 'L',\n",
       " 'ichens ',\n",
       " 'are ',\n",
       " 'also',\n",
       " ' ',\n",
       " 'import',\n",
       " 'ant',\n",
       " ' ',\n",
       " 'in',\n",
       " 'd',\n",
       " 'ic',\n",
       " 'at',\n",
       " 'or',\n",
       " 's of ',\n",
       " 'a',\n",
       " 'ir ',\n",
       " 'qu',\n",
       " 'al',\n",
       " 'ity',\n",
       " '. The',\n",
       " 'y ',\n",
       " 'are ',\n",
       " 'h',\n",
       " 'i',\n",
       " 'g',\n",
       " 'h',\n",
       " 'ly ',\n",
       " 's',\n",
       " 'en',\n",
       " 's',\n",
       " 'it',\n",
       " 'ive ',\n",
       " 'to ',\n",
       " 'p',\n",
       " 'ol',\n",
       " 'l',\n",
       " 'ut',\n",
       " 'ant',\n",
       " 's, ',\n",
       " 's',\n",
       " 'u',\n",
       " 'ch',\n",
       " ' ',\n",
       " 'as',\n",
       " ' s',\n",
       " 'u',\n",
       " 'l',\n",
       " 'f',\n",
       " 'ur',\n",
       " ' ',\n",
       " 'd',\n",
       " 'i',\n",
       " 'o',\n",
       " 'x',\n",
       " 'id',\n",
       " 'e ',\n",
       " 'and ',\n",
       " 'n',\n",
       " 'it',\n",
       " 'ro',\n",
       " 'g',\n",
       " 'en ',\n",
       " 'o',\n",
       " 'x',\n",
       " 'id',\n",
       " 'es. ',\n",
       " 'W',\n",
       " 'h',\n",
       " 'en ',\n",
       " 'th',\n",
       " 'es',\n",
       " 'e p',\n",
       " 'ol',\n",
       " 'l',\n",
       " 'ut',\n",
       " 'ant',\n",
       " 's ',\n",
       " 'ar',\n",
       " 'e p',\n",
       " 'r',\n",
       " 'es',\n",
       " 'ent',\n",
       " ' ',\n",
       " 'in ',\n",
       " 'the ',\n",
       " 'a',\n",
       " 'ir',\n",
       " ', ',\n",
       " 'lichens ',\n",
       " 'c',\n",
       " 'a']"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encode(test_data,merges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "da23697f-339f-423f-bc52-1ec90fc9daf5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{('e', ' '): 'e ',\n",
       " ('t', 'h'): 'th',\n",
       " ('t', ' '): 't ',\n",
       " ('s', ' '): 's ',\n",
       " ('.', '\\n'): '.\\n',\n",
       " (',', ' '): ', ',\n",
       " ('d', ' '): 'd ',\n",
       " ('e', 'r'): 'er',\n",
       " ('o', 'u'): 'ou',\n",
       " ('i', 'n'): 'in',\n",
       " ('a', 'n'): 'an',\n",
       " ('y', ' '): 'y ',\n",
       " ('o', 'r'): 'or',\n",
       " ('o', ' '): 'o ',\n",
       " ('e', 'n'): 'en',\n",
       " ('a', 'r'): 'ar',\n",
       " ('o', 'n'): 'on',\n",
       " ('l', 'l'): 'll',\n",
       " (' ', 'th'): ' th',\n",
       " ('h', 'a'): 'ha'}"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7027ede1-8744-445c-90c7-5166965a1b20",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bae94aa0-bde4-464d-82ce-70b041dd560a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb7f4186-ad25-4009-b5ca-4ae0c5a87883",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "phd",
   "language": "python",
   "name": "phd"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
