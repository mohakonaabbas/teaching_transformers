{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d7396c46",
   "metadata": {},
   "source": [
    "# In this notebook, we gain an intuition of training transformers and try to see it's pros and cons "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9ba12068",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "# Create the models\n",
    "\n",
    "import math\n",
    "import torch.nn.functional as F\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e81b155",
   "metadata": {},
   "source": [
    "### Create a Next character level prediction dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6ce610fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NextCharDataset(torch.utils.data.Dataset):\n",
    "\n",
    "    def __init__(self, dataset_path , context_length):\n",
    "        self.dataset_path = dataset_path\n",
    "        self.context_length = context_length\n",
    "        \n",
    "        with open(self.dataset_path,\"r\") as f:\n",
    "            self.raw_text = f.read()\n",
    "\n",
    "        self.tokens = list(self.raw_text)\n",
    "        \n",
    "        # Remap the tokens\n",
    "        \n",
    "        remapping = np.arange(len(set(self.tokens))).tolist()\n",
    "        self.mapping = dict(zip(list(set(self.tokens)),remapping))\n",
    "        self.inverse_mapping = dict(zip(remapping,list(set(self.tokens))))\n",
    "        \n",
    "        self.vocab_size = len(set(self.tokens))\n",
    "        \n",
    "        \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.tokens)-self.context_length\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        x_tokens = self.tokens[idx:idx+self.context_length+1]\n",
    "        x_maped = [self.mapping[i] for i in x_tokens]\n",
    "        x = x_maped[:-1]\n",
    "        y = x_maped[1:]\n",
    "\n",
    "        return torch.tensor(x,dtype=torch.long), torch.tensor(y,dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a83a6112-0f38-4df5-84ac-6adb7459b08f",
   "metadata": {},
   "outputs": [],
   "source": [
    "txtx= \"toto titi\"\n",
    "list(txtx)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8abf1fbb",
   "metadata": {},
   "source": [
    "### Create a next Token prediction dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8264135",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NextTokenDataset(torch.utils.data.Dataset):\n",
    "\n",
    "    def __init__(self, dataset_path , context_length):\n",
    "        self.dataset_path = dataset_path\n",
    "        self.context_length = context_length\n",
    "        \n",
    "        with open(self.dataset_path,\"r\") as f:\n",
    "            self.raw_text = f.read()\n",
    "        self.tokeniser = tiktoken.get_encoding(\"o200k_base\")\n",
    "        self.tokens = self.tokeniser.encode(self.raw_text)\n",
    "        \n",
    "        # Remap the tokens\n",
    "        \n",
    "        remapping = np.arange(len(set(self.tokens))).tolist()\n",
    "        self.mapping = dict(zip(list(set(self.tokens)),remapping))\n",
    "        self.inverse_mapping = dict(zip(remapping,list(set(self.tokens))))\n",
    "        \n",
    "        self.vocab_size = len(set(self.tokens))\n",
    "        \n",
    "        \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.tokens)-self.context_length\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        x_tokens = self.tokens[idx:idx+self.context_length+1]\n",
    "        x_maped = [self.mapping[i] for i in x_tokens]\n",
    "        x = x_maped[:-1]\n",
    "        y = x_maped[1:]\n",
    "\n",
    "        return torch.tensor(x,dtype=torch.long), torch.tensor(y,dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f79d1776",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "050955f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_datasets(dataset_class, dataset_path, context_length):\n",
    "    full_dataset = dataset_class(dataset_path, context_length)\n",
    "    train_dataset ,  test_dataset = torch.utils.data.random_split(full_dataset, [0.99,0.01])\n",
    "    \n",
    "    return train_dataset, test_dataset\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e0fb431",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc6590cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset, test_dataset = create_datasets(NextCharDataset,\"hymns.txt\", 100)\n",
    "batch_size = 100\n",
    "shuffle = True\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset,batch_size=batch_size, shuffle=shuffle)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset,batch_size=batch_size, shuffle=shuffle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73c2fad9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca122ef7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class RNNModel(nn.Module):\n",
    "    \"\"\"Container module with an encoder, a recurrent module, and a decoder.\"\"\"\n",
    "\n",
    "    def __init__(self,ntoken, ninp, nhid, nlayers, dropout=0.0):\n",
    "        super(RNNModel, self).__init__()\n",
    "        self.ntoken = ntoken\n",
    "        self.drop = nn.Dropout(dropout)\n",
    "        self.input_emb = nn.Embedding(ntoken, ninp)\n",
    "        self.rnn = nn.LSTM(ninp, nhid, nlayers, dropout=dropout,batch_first=True)\n",
    "        self.output_layer = nn.Linear(nhid, ntoken)\n",
    "        self.init_weights()\n",
    "\n",
    "        self.nhid = nhid\n",
    "        self.nlayers = nlayers\n",
    "\n",
    "    def init_weights(self):\n",
    "        initrange = 0.1\n",
    "        nn.init.uniform_(self.input_emb.weight, -initrange, initrange)\n",
    "        nn.init.zeros_(self.output_layer.bias)\n",
    "        nn.init.uniform_(self.output_layer.weight, -initrange, initrange)\n",
    "\n",
    "    def forward(self, x):\n",
    "        bs,context = x.size()\n",
    "        hidden = self.init_hidden(bs)\n",
    "        emb = self.drop(self.input_emb(x))\n",
    "        output, hidden = self.rnn(emb, hidden)\n",
    "        output = self.drop(output)\n",
    "        decoded = self.output_layer(output)\n",
    "\n",
    "        decoded = F.log_softmax(decoded, dim=1)\n",
    "\n",
    "        return decoded\n",
    "\n",
    "    def init_hidden(self, bsz):\n",
    "        weight = next(self.parameters())\n",
    "        return (weight.new_zeros(self.nlayers, bsz, self.nhid),\n",
    "                weight.new_zeros(self.nlayers, bsz, self.nhid))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cd4143c",
   "metadata": {},
   "source": [
    "### Positionnal Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24044d7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Temporarily leave PositionalEncoding module here. Will be moved somewhere else.\n",
    "class PositionalEncoding(nn.Module):\n",
    "    r\"\"\"Inject some information about the relative or absolute position of the tokens in the sequence.\n",
    "        The positional encodings have the same dimension as the embeddings, so that the two can be summed.\n",
    "        Here, we use sine and cosine functions of different frequencies.\n",
    "    .. math:\n",
    "        \\text{PosEncoder}(pos, 2i) = sin(pos/10000^(2i/d_model))\n",
    "        \\text{PosEncoder}(pos, 2i+1) = cos(pos/10000^(2i/d_model))\n",
    "        \\text{where pos is the word position and i is the embed idx)\n",
    "    Args:\n",
    "        d_model: the embed dim (required).\n",
    "        dropout: the dropout value (default=0.1).\n",
    "        max_len: the max. length of the incoming sequence (default=5000).\n",
    "    Examples:\n",
    "        >>> pos_encoder = PositionalEncoding(d_model)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, d_model, dropout=0.1, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        r\"\"\"Inputs of forward function\n",
    "        Args:\n",
    "            x: the sequence fed to the positional encoder model (required).\n",
    "        Shape:\n",
    "            x: [sequence length, batch size, embed dim]\n",
    "            output: [sequence length, batch size, embed dim]\n",
    "        Examples:\n",
    "            >>> output = pos_encoder(x)\n",
    "        \"\"\"\n",
    "\n",
    "        x = x + self.pe[:x.size(0), :]\n",
    "        return self.dropout(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff82919f",
   "metadata": {},
   "source": [
    "### Build your transformer model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cda44d2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class TransformerModel(nn.Transformer):\n",
    "    \"\"\"Container module with an encoder, a recurrent or transformer module, and a decoder.\"\"\"\n",
    "\n",
    "    def __init__(self, \n",
    "                 ntoken,\n",
    "                 ninp,\n",
    "                 nhead,\n",
    "                 nhid,\n",
    "                 nlayers,\n",
    "                 dropout=0.5):\n",
    "        super(TransformerModel, self).__init__(d_model=ninp, nhead=nhead, dim_feedforward=nhid, num_encoder_layers=nlayers)\n",
    "        self.model_type = 'Transformer'\n",
    "        self.src_mask = None\n",
    "        self.pos_encoder = PositionalEncoding(ninp, dropout)\n",
    "\n",
    "        self.input_emb = nn.Embedding(ntoken, ninp)\n",
    "        self.ninp = ninp\n",
    "        self.output_layer = nn.Linear(ninp, ntoken)\n",
    "\n",
    "        self.init_weights()\n",
    "\n",
    "    def _generate_square_subsequent_mask(self, sz):\n",
    "        return torch.log(torch.tril(torch.ones(sz,sz)))\n",
    "\n",
    "    def init_weights(self):\n",
    "        initrange = 0.1\n",
    "        nn.init.uniform_(self.input_emb.weight, -initrange, initrange)\n",
    "        nn.init.zeros_(self.output_layer.bias)\n",
    "        nn.init.uniform_(self.output_layer.weight, -initrange, initrange)\n",
    "\n",
    "    def forward(self, src, has_mask=True):\n",
    "\n",
    "        device = src.device\n",
    "\n",
    "        mask = self._generate_square_subsequent_mask(len(src)).to(device)\n",
    "        self.src_mask = mask\n",
    "\n",
    "\n",
    "        src = self.input_emb(src)\n",
    "        src = self.pos_encoder(src)\n",
    "        output = self.encoder(src, mask=self.src_mask)\n",
    "        output = self.output_layer(output)\n",
    "        return F.log_softmax(output, dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f1d92289",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "effd6393",
   "metadata": {},
   "outputs": [],
   "source": [
    "ntoken = train_dataset.dataset.vocab_size\n",
    "nembd = 64\n",
    "nhead = 4\n",
    "nlayer = 5\n",
    "feedforward_nlayer = 50\n",
    "lstm_nlayer = 5\n",
    "lstm_nhid = 50\n",
    "\n",
    "transformer_decoder = TransformerModel(ntoken,nembd,nhead ,feedforward_nlayer,nlayer )\n",
    "out = transformer_decoder(torch.tensor([[10,0,5,3,6,20,5]]))\n",
    "out.shape \n",
    "lstm_model = RNNModel(ntoken, nembd, lstm_nhid, lstm_nlayer, dropout=0.50).to(\"cuda\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97503839",
   "metadata": {},
   "source": [
    "###  Write the training loop for the transformer and the recurrent neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7309d54e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = transformer_decoder.to(\"cuda\")\n",
    "# model = lstm_model.to(\"cuda\")\n",
    "# init optimizer\n",
    "\n",
    "weight_decay = 0.01\n",
    "epochs = 150\n",
    "\n",
    "\n",
    "# training loop\n",
    "best_loss = None\n",
    "step = 0\n",
    "\n",
    "#for name, model , lr in zip([ \"lstm\"],[lstm_model],[5e-2]):\n",
    "for name, model , lr in zip([ \"transformer\",\"lstm\"],[transformer_decoder, lstm_model],[5e-3,5e-2]):\n",
    "    \n",
    "    final_lr = 5e-5\n",
    "    gamma = (final_lr / lr) ** (2.0 / epochs)\n",
    "    \n",
    "\n",
    "#TO DO \n",
    "\n",
    "    \n",
    "    print(f\"--\"*89)\n",
    "    print(f\"Model {name} | lr {lr}\")\n",
    "    print(\"--\" * 89)\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        t0 = time.time()\n",
    "        #TO DO\n",
    "\n",
    "\n",
    "        t1 = time.time()\n",
    "        print(f\"Epoch {epoch + 1} | Loss {loss.item():.4f} | Step time {(t1 - t0) * 1000:.2f}ms | Current LR {scheduler.get_last_lr()[0]:.6f}\")\n",
    "\n",
    "        #print(f\"Epoch {epoch} | loss {loss.item():.4f} | step time {(t1-t0)*1000:.2f}ms\")\n",
    "    print(f\"Model {name} | loss {loss.item():.4f} | step time {(t1-t0)*1000:.2f}ms\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dbff4ca",
   "metadata": {},
   "source": [
    "### Write the generation code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "450373a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def generate(model, idx, max_new_tokens, context, temperature=1.0, do_sample=False, top_k=None):\n",
    "    \"\"\"\n",
    "    Take a conditioning sequence of indices idx (LongTensor of shape (b,t)) and complete\n",
    "    the sequence max_new_tokens times, feeding the predictions back into the model each time.\n",
    "    Most likely you'll want to make sure to be in model.eval() mode of operation for this.\n",
    "    \"\"\"\n",
    "    block_size = context\n",
    "    for _ in range(max_new_tokens):\n",
    "        # if the sequence context is growing too long we must crop it at block_size\n",
    "        idx_cond = idx if idx.size(1) <= block_size else idx[:, -block_size:]\n",
    "        # forward the model to get the logits for the index in the sequence\n",
    "        logits= model(idx_cond)\n",
    "        # pluck the logits at the final step and scale by desired temperature\n",
    "\n",
    "        logits = logits[:, -1, :] / temperature\n",
    "        # optionally crop the logits to only the top k options\n",
    "        if top_k is not None:\n",
    "            v, _ = torch.topk(logits, top_k)\n",
    "            logits[logits < v[:, [-1]]] = -float('Inf')\n",
    "        # apply softmax to convert logits to (normalized) probabilities\n",
    "        probs = F.softmax(logits, dim=-1)\n",
    "        # either sample from the distribution or take the most likely element\n",
    "        if do_sample:\n",
    "            idx_next = torch.multinomial(probs, num_samples=1)\n",
    "        else:\n",
    "            _, idx_next = torch.topk(probs, k=1, dim=-1)\n",
    "        # append sampled index to the running sequence and continue\n",
    "        idx = torch.cat((idx, idx_next), dim=1)\n",
    "\n",
    "    return idx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f80035f5",
   "metadata": {},
   "source": [
    "### Generate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96266b7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\" vive \"\"\"\n",
    "\n",
    "prompt_tokens = train_dataset.dataset.tokeniser.encode(prompt)\n",
    "tokens = [train_dataset.dataset.mapping[i] for i in prompt_tokens]\n",
    "tokens = torch.tensor([tokens],dtype=torch.long)\n",
    "\n",
    "for name, model in zip([\"transformer\", \"lstm\"],[transformer_decoder, lstm_model]):\n",
    "    generated = generate(model,tokens.to(\"cuda\"),1000,context=10)\n",
    "    response = [train_dataset.dataset.inverse_mapping[i] for i in generated.cpu().numpy().squeeze().tolist()]\n",
    "    response = train_dataset.dataset.tokeniser.decode(response)\n",
    "    print(\"==\"*50)\n",
    "    print(f\"Name : {name}\")\n",
    "    print(\"==\"*50)\n",
    "    print(response)\n",
    "                      \n",
    "                      \n",
    "                      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "823bad53",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84de6ca2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6456c3b2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "transformers",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
