{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d089dfc3-ed80-45aa-90fc-f35bfe26e8de",
   "metadata": {},
   "source": [
    "# In this notebook, we gain an intuition of training transformers and try to see it's pros and cons "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0cb6b7fa-60a7-46cf-8720-a7d3b52f2001",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0071519-32a7-4f17-9440-b2487b15f3fa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bc4a0b4d",
   "metadata": {},
   "source": [
    "### Create a next Token prediction dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3d87d6d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NextTokenDataset(torch.utils.data.Dataset):\n",
    "\n",
    "    def __init__(self, dataset_path , context_length):\n",
    "        self.dataset_path = dataset_path\n",
    "        self.context_length = context_length\n",
    "        \n",
    "        with open(self.dataset_path,\"r\") as f:\n",
    "            self.raw_text = f.read()\n",
    "        self.tokeniser = tiktoken.get_encoding(\"o200k_base\")\n",
    "        self.tokens = self.tokeniser.encode(self.raw_text)\n",
    "        \n",
    "        # Remap the tokens\n",
    "        \n",
    "        remapping = np.arange(len(set(self.tokens))).tolist()\n",
    "        self.mapping = dict(zip(list(set(self.tokens)),remapping))\n",
    "        self.inverse_mapping = dict(zip(remapping,list(set(self.tokens))))\n",
    "        \n",
    "        self.vocab_size = len(set(self.tokens))\n",
    "        \n",
    "        \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.tokens)-self.context_length\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        x_tokens = self.tokens[idx:idx+self.context_length+1]\n",
    "        x_maped = [self.mapping[i] for i in x_tokens]\n",
    "        x = x_maped[:-1]\n",
    "        y = x_maped[1:]\n",
    "\n",
    "        return torch.tensor(x,dtype=torch.long), torch.tensor(y,dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "97c52f58",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_datasets(dataset_path, context_length):\n",
    "    full_dataset = NextTokenDataset(dataset_path, context_length)\n",
    "    train_dataset ,  test_dataset = torch.utils.data.random_split(full_dataset, [0.99,0.01])\n",
    "    \n",
    "    return train_dataset, test_dataset\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ab62a7a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the models\n",
    "\n",
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class RNNModel(nn.Module):\n",
    "    \"\"\"Container module with an encoder, a recurrent module, and a decoder.\"\"\"\n",
    "\n",
    "    def __init__(self,ntoken, ninp, nhid, nlayers, dropout=0.0):\n",
    "        super(RNNModel, self).__init__()\n",
    "        self.ntoken = ntoken\n",
    "        self.drop = nn.Dropout(dropout)\n",
    "        self.input_emb = nn.Embedding(ntoken, ninp)\n",
    "        self.rnn = nn.LSTM(ninp, nhid, nlayers, dropout=dropout,batch_first=True)\n",
    "        self.output_layer = nn.Linear(nhid, ntoken)\n",
    "        self.init_weights()\n",
    "\n",
    "        self.nhid = nhid\n",
    "        self.nlayers = nlayers\n",
    "\n",
    "    def init_weights(self):\n",
    "        initrange = 0.1\n",
    "        nn.init.uniform_(self.input_emb.weight, -initrange, initrange)\n",
    "        nn.init.zeros_(self.output_layer.bias)\n",
    "        nn.init.uniform_(self.output_layer.weight, -initrange, initrange)\n",
    "\n",
    "    def forward(self, x):\n",
    "        bs,context = x.size()\n",
    "        hidden = self.init_hidden(bs)\n",
    "        emb = self.drop(self.input_emb(x))\n",
    "        output, hidden = self.rnn(emb, hidden)\n",
    "        output = self.drop(output)\n",
    "        decoded = self.output_layer(output)\n",
    "\n",
    "        decoded = F.log_softmax(decoded, dim=1)\n",
    "\n",
    "        return decoded\n",
    "\n",
    "    def init_hidden(self, bsz):\n",
    "        weight = next(self.parameters())\n",
    "        return (weight.new_zeros(self.nlayers, bsz, self.nhid),\n",
    "                weight.new_zeros(self.nlayers, bsz, self.nhid))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a113d301",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Temporarily leave PositionalEncoding module here. Will be moved somewhere else.\n",
    "class PositionalEncoding(nn.Module):\n",
    "    r\"\"\"Inject some information about the relative or absolute position of the tokens in the sequence.\n",
    "        The positional encodings have the same dimension as the embeddings, so that the two can be summed.\n",
    "        Here, we use sine and cosine functions of different frequencies.\n",
    "    .. math:\n",
    "        \\text{PosEncoder}(pos, 2i) = sin(pos/10000^(2i/d_model))\n",
    "        \\text{PosEncoder}(pos, 2i+1) = cos(pos/10000^(2i/d_model))\n",
    "        \\text{where pos is the word position and i is the embed idx)\n",
    "    Args:\n",
    "        d_model: the embed dim (required).\n",
    "        dropout: the dropout value (default=0.1).\n",
    "        max_len: the max. length of the incoming sequence (default=5000).\n",
    "    Examples:\n",
    "        >>> pos_encoder = PositionalEncoding(d_model)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, d_model, dropout=0.1, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        r\"\"\"Inputs of forward function\n",
    "        Args:\n",
    "            x: the sequence fed to the positional encoder model (required).\n",
    "        Shape:\n",
    "            x: [sequence length, batch size, embed dim]\n",
    "            output: [sequence length, batch size, embed dim]\n",
    "        Examples:\n",
    "            >>> output = pos_encoder(x)\n",
    "        \"\"\"\n",
    "\n",
    "        x = x + self.pe[:x.size(0), :]\n",
    "        return self.dropout(x)\n",
    "\n",
    "class TransformerModel(nn.Transformer):\n",
    "    \"\"\"Container module with an encoder, a recurrent or transformer module, and a decoder.\"\"\"\n",
    "\n",
    "    def __init__(self, \n",
    "                 ntoken,\n",
    "                 ninp,\n",
    "                 nhead,\n",
    "                 nhid,\n",
    "                 nlayers,\n",
    "                 dropout=0.5):\n",
    "        super(TransformerModel, self).__init__(d_model=ninp, nhead=nhead, dim_feedforward=nhid, num_encoder_layers=nlayers)\n",
    "        self.model_type = 'Transformer'\n",
    "        self.src_mask = None\n",
    "        self.pos_encoder = PositionalEncoding(ninp, dropout)\n",
    "\n",
    "        self.input_emb = nn.Embedding(ntoken, ninp)\n",
    "        self.ninp = ninp\n",
    "        self.output_layer = nn.Linear(ninp, ntoken)\n",
    "\n",
    "        self.init_weights()\n",
    "\n",
    "    def _generate_square_subsequent_mask(self, sz):\n",
    "        return torch.log(torch.tril(torch.ones(sz,sz)))\n",
    "\n",
    "    def init_weights(self):\n",
    "        initrange = 0.1\n",
    "        nn.init.uniform_(self.input_emb.weight, -initrange, initrange)\n",
    "        nn.init.zeros_(self.output_layer.bias)\n",
    "        nn.init.uniform_(self.output_layer.weight, -initrange, initrange)\n",
    "\n",
    "    def forward(self, src, has_mask=True):\n",
    "\n",
    "        device = src.device\n",
    "\n",
    "        mask = self._generate_square_subsequent_mask(len(src)).to(device)\n",
    "        self.src_mask = mask\n",
    "\n",
    "\n",
    "        src = self.input_emb(src)\n",
    "        src = self.pos_encoder(src)\n",
    "        output = self.encoder(src, mask=self.src_mask)\n",
    "        output = self.output_layer(output)\n",
    "        return F.log_softmax(output, dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "59fb3021",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset, test_dataset = create_datasets(\"hymns.txt\", 100)\n",
    "batch_size = 100\n",
    "shuffle = True\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset,batch_size=batch_size, shuffle=shuffle)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset,batch_size=batch_size, shuffle=shuffle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0938fa31",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/msi/miniconda3/envs/transformers/lib/python3.12/site-packages/torch/nn/modules/transformer.py:385: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "ntoken = train_dataset.dataset.vocab_size\n",
    "nembd = 64\n",
    "nhead = 4\n",
    "nlayer = 5\n",
    "feedforward_nlayer = 50\n",
    "lstm_nlayer = 5\n",
    "lstm_nhid = 50\n",
    "\n",
    "transformer_decoder = TransformerModel(ntoken,nembd,nhead ,feedforward_nlayer,nlayer )\n",
    "out = transformer_decoder(torch.tensor([[10,0,5,3,6,20,5]]))\n",
    "out.shape \n",
    "lstm_model = RNNModel(ntoken, nembd, lstm_nhid, lstm_nlayer, dropout=0.50).to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "462396c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "Model transformer | lr 0.005\n",
      "----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "Epoch 1 | Loss 6.4474 | Step time 10348.26ms | Current LR 0.004702\n",
      "Epoch 2 | Loss 6.4770 | Step time 9537.97ms | Current LR 0.004422\n",
      "Epoch 3 | Loss 6.4294 | Step time 9215.79ms | Current LR 0.004159\n",
      "Epoch 4 | Loss 6.5253 | Step time 8502.95ms | Current LR 0.003911\n",
      "Epoch 5 | Loss 6.4365 | Step time 9251.13ms | Current LR 0.003678\n",
      "Epoch 6 | Loss 6.4375 | Step time 6969.94ms | Current LR 0.003459\n",
      "Epoch 7 | Loss 5.9008 | Step time 8487.13ms | Current LR 0.003253\n",
      "Epoch 8 | Loss 5.5363 | Step time 7081.63ms | Current LR 0.003059\n",
      "Epoch 9 | Loss 5.2737 | Step time 6523.35ms | Current LR 0.002877\n",
      "Epoch 10 | Loss 5.1787 | Step time 7507.89ms | Current LR 0.002706\n",
      "Epoch 11 | Loss 4.9962 | Step time 7867.11ms | Current LR 0.002545\n",
      "Epoch 12 | Loss 4.8007 | Step time 7217.68ms | Current LR 0.002393\n",
      "Epoch 13 | Loss 4.5601 | Step time 8025.62ms | Current LR 0.002251\n",
      "Epoch 14 | Loss 4.1753 | Step time 6337.86ms | Current LR 0.002117\n",
      "Epoch 15 | Loss 3.7900 | Step time 6670.73ms | Current LR 0.001991\n",
      "Epoch 16 | Loss 3.4848 | Step time 6074.84ms | Current LR 0.001872\n",
      "Epoch 17 | Loss 3.2015 | Step time 7260.25ms | Current LR 0.001761\n",
      "Epoch 18 | Loss 2.9579 | Step time 7318.51ms | Current LR 0.001656\n",
      "Epoch 19 | Loss 2.7811 | Step time 6999.02ms | Current LR 0.001557\n",
      "Epoch 20 | Loss 2.7032 | Step time 6744.30ms | Current LR 0.001464\n",
      "Epoch 21 | Loss 2.5328 | Step time 8071.62ms | Current LR 0.001377\n",
      "Epoch 22 | Loss 2.5141 | Step time 5626.70ms | Current LR 0.001295\n",
      "Epoch 23 | Loss 2.4604 | Step time 6556.96ms | Current LR 0.001218\n",
      "Epoch 24 | Loss 2.3991 | Step time 6862.95ms | Current LR 0.001145\n",
      "Epoch 25 | Loss 2.2878 | Step time 7128.71ms | Current LR 0.001077\n",
      "Epoch 26 | Loss 2.2867 | Step time 8158.63ms | Current LR 0.001013\n",
      "Epoch 27 | Loss 2.2539 | Step time 6526.97ms | Current LR 0.000953\n",
      "Epoch 28 | Loss 2.2781 | Step time 8277.70ms | Current LR 0.000896\n",
      "Epoch 29 | Loss 2.2709 | Step time 7888.16ms | Current LR 0.000843\n",
      "Epoch 30 | Loss 2.2064 | Step time 8197.94ms | Current LR 0.000792\n",
      "Epoch 31 | Loss 2.2061 | Step time 9289.69ms | Current LR 0.000745\n",
      "Epoch 32 | Loss 2.1933 | Step time 8691.19ms | Current LR 0.000701\n",
      "Epoch 33 | Loss 2.1963 | Step time 7527.14ms | Current LR 0.000659\n",
      "Epoch 34 | Loss 2.1869 | Step time 7432.92ms | Current LR 0.000620\n",
      "Epoch 35 | Loss 2.1489 | Step time 8209.67ms | Current LR 0.000583\n",
      "Epoch 36 | Loss 2.1533 | Step time 7348.88ms | Current LR 0.000548\n",
      "Epoch 37 | Loss 2.1486 | Step time 7837.46ms | Current LR 0.000516\n",
      "Epoch 38 | Loss 2.1290 | Step time 8074.77ms | Current LR 0.000485\n",
      "Epoch 39 | Loss 2.1700 | Step time 6600.57ms | Current LR 0.000456\n",
      "Epoch 40 | Loss 2.1632 | Step time 7269.19ms | Current LR 0.000429\n",
      "Epoch 41 | Loss 2.1166 | Step time 7386.53ms | Current LR 0.000403\n",
      "Epoch 42 | Loss 2.1128 | Step time 7397.76ms | Current LR 0.000379\n",
      "Epoch 43 | Loss 2.1026 | Step time 7282.91ms | Current LR 0.000357\n",
      "Epoch 44 | Loss 2.1216 | Step time 7408.69ms | Current LR 0.000335\n",
      "Epoch 45 | Loss 2.1302 | Step time 7613.64ms | Current LR 0.000315\n",
      "Epoch 46 | Loss 2.0768 | Step time 6513.50ms | Current LR 0.000297\n",
      "Epoch 47 | Loss 2.1114 | Step time 6882.94ms | Current LR 0.000279\n",
      "Epoch 48 | Loss 2.0614 | Step time 8534.12ms | Current LR 0.000262\n",
      "Epoch 49 | Loss 2.0679 | Step time 6079.51ms | Current LR 0.000247\n",
      "Epoch 50 | Loss 2.1024 | Step time 7250.45ms | Current LR 0.000232\n",
      "Epoch 51 | Loss 2.0510 | Step time 8524.32ms | Current LR 0.000218\n",
      "Epoch 52 | Loss 2.0877 | Step time 7154.09ms | Current LR 0.000205\n",
      "Epoch 53 | Loss 2.1109 | Step time 5969.42ms | Current LR 0.000193\n",
      "Epoch 54 | Loss 2.1010 | Step time 7330.21ms | Current LR 0.000182\n",
      "Epoch 55 | Loss 2.0776 | Step time 8267.08ms | Current LR 0.000171\n",
      "Epoch 56 | Loss 2.1025 | Step time 7604.62ms | Current LR 0.000161\n",
      "Epoch 57 | Loss 2.0633 | Step time 6100.97ms | Current LR 0.000151\n",
      "Epoch 58 | Loss 2.0774 | Step time 6877.68ms | Current LR 0.000142\n",
      "Epoch 59 | Loss 2.0816 | Step time 7333.84ms | Current LR 0.000134\n",
      "Epoch 60 | Loss 2.0824 | Step time 6285.90ms | Current LR 0.000126\n",
      "Epoch 61 | Loss 2.0946 | Step time 7393.29ms | Current LR 0.000118\n",
      "Epoch 62 | Loss 2.0610 | Step time 8389.58ms | Current LR 0.000111\n",
      "Epoch 63 | Loss 2.0518 | Step time 7930.13ms | Current LR 0.000104\n",
      "Epoch 64 | Loss 2.0834 | Step time 7154.61ms | Current LR 0.000098\n",
      "Epoch 65 | Loss 2.0778 | Step time 7075.22ms | Current LR 0.000092\n",
      "Epoch 66 | Loss 2.0702 | Step time 6900.22ms | Current LR 0.000087\n",
      "Epoch 67 | Loss 2.0436 | Step time 7074.21ms | Current LR 0.000082\n",
      "Epoch 68 | Loss 2.0745 | Step time 8050.03ms | Current LR 0.000077\n",
      "Epoch 69 | Loss 2.0743 | Step time 6779.30ms | Current LR 0.000072\n",
      "Epoch 70 | Loss 2.0640 | Step time 7683.34ms | Current LR 0.000068\n",
      "Epoch 71 | Loss 2.0657 | Step time 7355.98ms | Current LR 0.000064\n",
      "Epoch 72 | Loss 2.0652 | Step time 6663.94ms | Current LR 0.000060\n",
      "Epoch 73 | Loss 2.0721 | Step time 7520.88ms | Current LR 0.000057\n",
      "Epoch 74 | Loss 2.1418 | Step time 7311.56ms | Current LR 0.000053\n",
      "Epoch 75 | Loss 2.0445 | Step time 8078.33ms | Current LR 0.000050\n",
      "Epoch 76 | Loss 2.0777 | Step time 8397.53ms | Current LR 0.000047\n",
      "Epoch 77 | Loss 2.0713 | Step time 7485.59ms | Current LR 0.000044\n",
      "Epoch 78 | Loss 2.0482 | Step time 7304.86ms | Current LR 0.000042\n",
      "Epoch 79 | Loss 2.0740 | Step time 7572.65ms | Current LR 0.000039\n",
      "Epoch 80 | Loss 2.0353 | Step time 8063.57ms | Current LR 0.000037\n",
      "Epoch 81 | Loss 2.0877 | Step time 7745.10ms | Current LR 0.000035\n",
      "Epoch 82 | Loss 2.0635 | Step time 7759.94ms | Current LR 0.000033\n",
      "Epoch 83 | Loss 2.0341 | Step time 6163.81ms | Current LR 0.000031\n",
      "Epoch 84 | Loss 2.0524 | Step time 8629.93ms | Current LR 0.000029\n",
      "Epoch 85 | Loss 2.0658 | Step time 8140.70ms | Current LR 0.000027\n",
      "Epoch 86 | Loss 2.0460 | Step time 5417.78ms | Current LR 0.000025\n",
      "Epoch 87 | Loss 2.0274 | Step time 5506.43ms | Current LR 0.000024\n",
      "Epoch 88 | Loss 2.0590 | Step time 5545.16ms | Current LR 0.000023\n",
      "Epoch 89 | Loss 2.0990 | Step time 6861.21ms | Current LR 0.000021\n",
      "Epoch 90 | Loss 2.0580 | Step time 6692.22ms | Current LR 0.000020\n",
      "Epoch 91 | Loss 2.0343 | Step time 7102.39ms | Current LR 0.000019\n",
      "Epoch 92 | Loss 2.0157 | Step time 6245.71ms | Current LR 0.000018\n",
      "Epoch 93 | Loss 2.0764 | Step time 7209.51ms | Current LR 0.000017\n",
      "Epoch 94 | Loss 2.0636 | Step time 6958.31ms | Current LR 0.000016\n",
      "Epoch 95 | Loss 2.0549 | Step time 6355.27ms | Current LR 0.000015\n",
      "Epoch 96 | Loss 2.1094 | Step time 7074.39ms | Current LR 0.000014\n",
      "Epoch 97 | Loss 2.0851 | Step time 6931.29ms | Current LR 0.000013\n",
      "Epoch 98 | Loss 2.0908 | Step time 7725.87ms | Current LR 0.000012\n",
      "Epoch 99 | Loss 2.0868 | Step time 6920.74ms | Current LR 0.000011\n",
      "Epoch 100 | Loss 2.0960 | Step time 7671.70ms | Current LR 0.000011\n",
      "Epoch 101 | Loss 2.0325 | Step time 7241.97ms | Current LR 0.000010\n",
      "Epoch 102 | Loss 2.0893 | Step time 6339.40ms | Current LR 0.000010\n",
      "Epoch 103 | Loss 2.0429 | Step time 5818.98ms | Current LR 0.000009\n",
      "Epoch 104 | Loss 2.1008 | Step time 6021.18ms | Current LR 0.000008\n",
      "Epoch 105 | Loss 2.0947 | Step time 5468.43ms | Current LR 0.000008\n",
      "Epoch 106 | Loss 2.0503 | Step time 5359.30ms | Current LR 0.000007\n",
      "Epoch 107 | Loss 2.0424 | Step time 6822.46ms | Current LR 0.000007\n",
      "Epoch 108 | Loss 2.0740 | Step time 6152.66ms | Current LR 0.000007\n",
      "Epoch 109 | Loss 2.1093 | Step time 7571.62ms | Current LR 0.000006\n",
      "Epoch 110 | Loss 2.0507 | Step time 5861.86ms | Current LR 0.000006\n",
      "Epoch 111 | Loss 2.0474 | Step time 7395.44ms | Current LR 0.000005\n",
      "Epoch 112 | Loss 2.0740 | Step time 6642.22ms | Current LR 0.000005\n",
      "Epoch 113 | Loss 2.0630 | Step time 6229.69ms | Current LR 0.000005\n",
      "Epoch 114 | Loss 2.0718 | Step time 6426.27ms | Current LR 0.000005\n",
      "Epoch 115 | Loss 2.0872 | Step time 7191.39ms | Current LR 0.000004\n",
      "Epoch 116 | Loss 2.0179 | Step time 6078.06ms | Current LR 0.000004\n",
      "Epoch 117 | Loss 2.0602 | Step time 5581.81ms | Current LR 0.000004\n",
      "Epoch 118 | Loss 2.0301 | Step time 5864.78ms | Current LR 0.000004\n",
      "Epoch 119 | Loss 2.1100 | Step time 5766.25ms | Current LR 0.000003\n",
      "Epoch 120 | Loss 2.1064 | Step time 8233.82ms | Current LR 0.000003\n",
      "Epoch 121 | Loss 2.0897 | Step time 6639.00ms | Current LR 0.000003\n",
      "Epoch 122 | Loss 2.0595 | Step time 6473.96ms | Current LR 0.000003\n",
      "Epoch 123 | Loss 2.0645 | Step time 6473.88ms | Current LR 0.000003\n",
      "Epoch 124 | Loss 2.0563 | Step time 7066.77ms | Current LR 0.000002\n",
      "Epoch 125 | Loss 2.0831 | Step time 6433.98ms | Current LR 0.000002\n",
      "Epoch 126 | Loss 2.0553 | Step time 5675.37ms | Current LR 0.000002\n",
      "Epoch 127 | Loss 2.0520 | Step time 6746.84ms | Current LR 0.000002\n",
      "Epoch 128 | Loss 2.0143 | Step time 7121.57ms | Current LR 0.000002\n",
      "Epoch 129 | Loss 2.0895 | Step time 5686.70ms | Current LR 0.000002\n",
      "Epoch 130 | Loss 2.0835 | Step time 7036.56ms | Current LR 0.000002\n",
      "Epoch 131 | Loss 2.0733 | Step time 6655.23ms | Current LR 0.000002\n",
      "Epoch 132 | Loss 2.0377 | Step time 5748.90ms | Current LR 0.000002\n",
      "Epoch 133 | Loss 2.0926 | Step time 6450.75ms | Current LR 0.000001\n",
      "Epoch 134 | Loss 2.0569 | Step time 9351.70ms | Current LR 0.000001\n",
      "Epoch 135 | Loss 2.0821 | Step time 8000.53ms | Current LR 0.000001\n",
      "Epoch 136 | Loss 2.0645 | Step time 6678.80ms | Current LR 0.000001\n",
      "Epoch 137 | Loss 2.0382 | Step time 7957.58ms | Current LR 0.000001\n",
      "Epoch 138 | Loss 2.0719 | Step time 7477.46ms | Current LR 0.000001\n",
      "Epoch 139 | Loss 2.0832 | Step time 8620.11ms | Current LR 0.000001\n",
      "Epoch 140 | Loss 2.0803 | Step time 6535.85ms | Current LR 0.000001\n",
      "Epoch 141 | Loss 2.0643 | Step time 7537.39ms | Current LR 0.000001\n",
      "Epoch 142 | Loss 2.0857 | Step time 6557.83ms | Current LR 0.000001\n",
      "Epoch 143 | Loss 2.0890 | Step time 7115.16ms | Current LR 0.000001\n",
      "Epoch 144 | Loss 2.0487 | Step time 7679.65ms | Current LR 0.000001\n",
      "Epoch 145 | Loss 2.0571 | Step time 7971.33ms | Current LR 0.000001\n",
      "Epoch 146 | Loss 2.0504 | Step time 8177.52ms | Current LR 0.000001\n",
      "Epoch 147 | Loss 2.0400 | Step time 6082.58ms | Current LR 0.000001\n",
      "Epoch 148 | Loss 2.0786 | Step time 7589.87ms | Current LR 0.000001\n",
      "Epoch 149 | Loss 2.0623 | Step time 8839.73ms | Current LR 0.000001\n",
      "Epoch 150 | Loss 2.0430 | Step time 5969.16ms | Current LR 0.000001\n",
      "Model transformer | loss 2.0430 | step time 5969.16ms\n",
      "----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "Model lstm | lr 0.05\n",
      "----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "Epoch 1 | Loss 7.5944 | Step time 2426.11ms | Current LR 0.045601\n",
      "Epoch 2 | Loss 7.5224 | Step time 991.63ms | Current LR 0.041588\n",
      "Epoch 3 | Loss 5.4241 | Step time 981.95ms | Current LR 0.037929\n",
      "Epoch 4 | Loss 4.6126 | Step time 920.27ms | Current LR 0.034592\n",
      "Epoch 5 | Loss 4.2440 | Step time 926.84ms | Current LR 0.031548\n",
      "Epoch 6 | Loss 4.1102 | Step time 963.36ms | Current LR 0.028772\n",
      "Epoch 7 | Loss 3.8012 | Step time 1219.99ms | Current LR 0.026240\n",
      "Epoch 8 | Loss 3.7860 | Step time 1058.72ms | Current LR 0.023932\n",
      "Epoch 9 | Loss 3.7956 | Step time 2175.10ms | Current LR 0.021826\n",
      "Epoch 10 | Loss 3.6411 | Step time 1085.52ms | Current LR 0.019905\n",
      "Epoch 11 | Loss 3.6758 | Step time 916.20ms | Current LR 0.018154\n",
      "Epoch 12 | Loss 3.4949 | Step time 905.27ms | Current LR 0.016557\n",
      "Epoch 13 | Loss 3.4361 | Step time 909.58ms | Current LR 0.015100\n",
      "Epoch 14 | Loss 3.5671 | Step time 910.29ms | Current LR 0.013771\n",
      "Epoch 15 | Loss 3.4236 | Step time 907.31ms | Current LR 0.012559\n",
      "Epoch 16 | Loss 3.2817 | Step time 905.18ms | Current LR 0.011454\n",
      "Epoch 17 | Loss 3.3464 | Step time 1395.69ms | Current LR 0.010446\n",
      "Epoch 18 | Loss 3.2171 | Step time 904.91ms | Current LR 0.009527\n",
      "Epoch 19 | Loss 3.1987 | Step time 910.25ms | Current LR 0.008689\n",
      "Epoch 20 | Loss 3.4157 | Step time 905.33ms | Current LR 0.007924\n",
      "Epoch 21 | Loss 3.3206 | Step time 937.36ms | Current LR 0.007227\n",
      "Epoch 22 | Loss 3.3239 | Step time 1431.81ms | Current LR 0.006591\n",
      "Epoch 23 | Loss 3.3414 | Step time 907.60ms | Current LR 0.006011\n",
      "Epoch 24 | Loss 3.2275 | Step time 1181.23ms | Current LR 0.005482\n",
      "Epoch 25 | Loss 3.3287 | Step time 910.57ms | Current LR 0.005000\n",
      "Epoch 26 | Loss 3.2792 | Step time 912.03ms | Current LR 0.004560\n",
      "Epoch 27 | Loss 3.2360 | Step time 912.26ms | Current LR 0.004159\n",
      "Epoch 28 | Loss 3.1615 | Step time 957.86ms | Current LR 0.003793\n",
      "Epoch 29 | Loss 3.1630 | Step time 960.26ms | Current LR 0.003459\n",
      "Epoch 30 | Loss 3.2266 | Step time 960.15ms | Current LR 0.003155\n",
      "Epoch 31 | Loss 3.2226 | Step time 940.74ms | Current LR 0.002877\n",
      "Epoch 32 | Loss 3.1889 | Step time 913.43ms | Current LR 0.002624\n",
      "Epoch 33 | Loss 3.1820 | Step time 911.55ms | Current LR 0.002393\n",
      "Epoch 34 | Loss 3.0905 | Step time 911.86ms | Current LR 0.002183\n",
      "Epoch 35 | Loss 3.2023 | Step time 907.05ms | Current LR 0.001991\n",
      "Epoch 36 | Loss 3.2051 | Step time 912.45ms | Current LR 0.001815\n",
      "Epoch 37 | Loss 3.1379 | Step time 906.26ms | Current LR 0.001656\n",
      "Epoch 38 | Loss 3.1877 | Step time 1027.18ms | Current LR 0.001510\n",
      "Epoch 39 | Loss 3.2410 | Step time 919.59ms | Current LR 0.001377\n",
      "Epoch 40 | Loss 3.1824 | Step time 911.84ms | Current LR 0.001256\n",
      "Epoch 41 | Loss 3.2088 | Step time 910.60ms | Current LR 0.001145\n",
      "Epoch 42 | Loss 3.2355 | Step time 1182.85ms | Current LR 0.001045\n",
      "Epoch 43 | Loss 3.0812 | Step time 918.16ms | Current LR 0.000953\n",
      "Epoch 44 | Loss 3.1441 | Step time 1195.75ms | Current LR 0.000869\n",
      "Epoch 45 | Loss 3.2101 | Step time 911.93ms | Current LR 0.000792\n",
      "Epoch 46 | Loss 3.2030 | Step time 909.42ms | Current LR 0.000723\n",
      "Epoch 47 | Loss 3.1645 | Step time 905.82ms | Current LR 0.000659\n",
      "Epoch 48 | Loss 3.2428 | Step time 1988.24ms | Current LR 0.000601\n",
      "Epoch 49 | Loss 3.1421 | Step time 2499.19ms | Current LR 0.000548\n",
      "Epoch 50 | Loss 3.1538 | Step time 2204.38ms | Current LR 0.000500\n",
      "Epoch 51 | Loss 3.1992 | Step time 873.87ms | Current LR 0.000456\n",
      "Epoch 52 | Loss 3.1072 | Step time 879.68ms | Current LR 0.000416\n",
      "Epoch 53 | Loss 3.0980 | Step time 878.67ms | Current LR 0.000379\n",
      "Epoch 54 | Loss 3.1703 | Step time 875.71ms | Current LR 0.000346\n",
      "Epoch 55 | Loss 3.1877 | Step time 874.59ms | Current LR 0.000315\n",
      "Epoch 56 | Loss 3.0966 | Step time 880.49ms | Current LR 0.000288\n",
      "Epoch 57 | Loss 3.1594 | Step time 878.87ms | Current LR 0.000262\n",
      "Epoch 58 | Loss 3.1343 | Step time 881.72ms | Current LR 0.000239\n",
      "Epoch 59 | Loss 3.2067 | Step time 875.84ms | Current LR 0.000218\n",
      "Epoch 60 | Loss 3.1293 | Step time 884.61ms | Current LR 0.000199\n",
      "Epoch 61 | Loss 3.1035 | Step time 876.48ms | Current LR 0.000182\n",
      "Epoch 62 | Loss 3.1553 | Step time 902.88ms | Current LR 0.000166\n",
      "Epoch 63 | Loss 3.1487 | Step time 915.71ms | Current LR 0.000151\n",
      "Epoch 64 | Loss 3.1781 | Step time 906.23ms | Current LR 0.000138\n",
      "Epoch 65 | Loss 3.1384 | Step time 910.78ms | Current LR 0.000126\n",
      "Epoch 66 | Loss 3.1102 | Step time 1536.39ms | Current LR 0.000115\n",
      "Epoch 67 | Loss 3.1805 | Step time 2500.30ms | Current LR 0.000104\n",
      "Epoch 68 | Loss 3.1551 | Step time 1370.37ms | Current LR 0.000095\n",
      "Epoch 69 | Loss 3.1204 | Step time 940.51ms | Current LR 0.000087\n",
      "Epoch 70 | Loss 3.1368 | Step time 944.55ms | Current LR 0.000079\n",
      "Epoch 71 | Loss 3.0921 | Step time 949.70ms | Current LR 0.000072\n",
      "Epoch 72 | Loss 3.2093 | Step time 945.10ms | Current LR 0.000066\n",
      "Epoch 73 | Loss 3.1344 | Step time 948.04ms | Current LR 0.000060\n",
      "Epoch 74 | Loss 3.0910 | Step time 957.20ms | Current LR 0.000055\n",
      "Epoch 75 | Loss 3.1697 | Step time 948.29ms | Current LR 0.000050\n",
      "Epoch 76 | Loss 3.1418 | Step time 964.96ms | Current LR 0.000046\n",
      "Epoch 77 | Loss 3.2215 | Step time 956.87ms | Current LR 0.000042\n",
      "Epoch 78 | Loss 3.0614 | Step time 990.74ms | Current LR 0.000038\n",
      "Epoch 79 | Loss 3.1691 | Step time 1096.45ms | Current LR 0.000035\n",
      "Epoch 80 | Loss 3.2546 | Step time 1010.31ms | Current LR 0.000032\n",
      "Epoch 81 | Loss 3.1186 | Step time 1003.99ms | Current LR 0.000029\n",
      "Epoch 82 | Loss 3.1223 | Step time 1972.16ms | Current LR 0.000026\n",
      "Epoch 83 | Loss 3.2026 | Step time 2494.85ms | Current LR 0.000024\n",
      "Epoch 84 | Loss 3.1607 | Step time 2493.93ms | Current LR 0.000022\n",
      "Epoch 85 | Loss 3.1041 | Step time 2493.99ms | Current LR 0.000020\n",
      "Epoch 86 | Loss 3.1285 | Step time 1891.32ms | Current LR 0.000018\n",
      "Epoch 87 | Loss 3.1808 | Step time 1320.52ms | Current LR 0.000017\n",
      "Epoch 88 | Loss 3.0731 | Step time 2489.00ms | Current LR 0.000015\n",
      "Epoch 89 | Loss 3.1569 | Step time 2382.38ms | Current LR 0.000014\n",
      "Epoch 90 | Loss 3.2610 | Step time 1633.08ms | Current LR 0.000013\n",
      "Epoch 91 | Loss 3.0904 | Step time 957.05ms | Current LR 0.000011\n",
      "Epoch 92 | Loss 3.2132 | Step time 957.08ms | Current LR 0.000010\n",
      "Epoch 93 | Loss 3.1342 | Step time 955.42ms | Current LR 0.000010\n",
      "Epoch 94 | Loss 3.2002 | Step time 968.98ms | Current LR 0.000009\n",
      "Epoch 95 | Loss 3.1136 | Step time 1023.61ms | Current LR 0.000008\n",
      "Epoch 96 | Loss 3.0881 | Step time 1033.03ms | Current LR 0.000007\n",
      "Epoch 97 | Loss 3.0583 | Step time 1034.03ms | Current LR 0.000007\n",
      "Epoch 98 | Loss 3.0895 | Step time 1032.15ms | Current LR 0.000006\n",
      "Epoch 99 | Loss 3.1280 | Step time 1051.88ms | Current LR 0.000005\n",
      "Epoch 100 | Loss 3.1764 | Step time 1050.95ms | Current LR 0.000005\n",
      "Epoch 101 | Loss 3.0600 | Step time 1176.94ms | Current LR 0.000005\n",
      "Epoch 102 | Loss 3.1685 | Step time 1083.17ms | Current LR 0.000004\n",
      "Epoch 103 | Loss 3.1232 | Step time 1043.05ms | Current LR 0.000004\n",
      "Epoch 104 | Loss 3.1315 | Step time 1100.22ms | Current LR 0.000003\n",
      "Epoch 105 | Loss 3.1902 | Step time 1050.29ms | Current LR 0.000003\n",
      "Epoch 106 | Loss 3.2799 | Step time 1063.45ms | Current LR 0.000003\n",
      "Epoch 107 | Loss 3.1112 | Step time 1025.47ms | Current LR 0.000003\n",
      "Epoch 108 | Loss 3.1487 | Step time 1209.23ms | Current LR 0.000002\n",
      "Epoch 109 | Loss 3.1385 | Step time 1083.82ms | Current LR 0.000002\n",
      "Epoch 110 | Loss 3.1323 | Step time 1083.69ms | Current LR 0.000002\n",
      "Epoch 111 | Loss 3.1702 | Step time 1070.50ms | Current LR 0.000002\n",
      "Epoch 112 | Loss 3.0602 | Step time 1041.46ms | Current LR 0.000002\n",
      "Epoch 113 | Loss 3.1498 | Step time 1049.61ms | Current LR 0.000002\n",
      "Epoch 114 | Loss 3.1253 | Step time 1339.28ms | Current LR 0.000001\n",
      "Epoch 115 | Loss 3.1551 | Step time 1724.80ms | Current LR 0.000001\n",
      "Epoch 116 | Loss 3.1138 | Step time 951.86ms | Current LR 0.000001\n",
      "Epoch 117 | Loss 3.1426 | Step time 960.36ms | Current LR 0.000001\n",
      "Epoch 118 | Loss 3.1138 | Step time 981.03ms | Current LR 0.000001\n",
      "Epoch 119 | Loss 3.1550 | Step time 1004.36ms | Current LR 0.000001\n",
      "Epoch 120 | Loss 3.1093 | Step time 994.68ms | Current LR 0.000001\n",
      "Epoch 121 | Loss 3.0597 | Step time 955.07ms | Current LR 0.000001\n",
      "Epoch 122 | Loss 3.2614 | Step time 949.82ms | Current LR 0.000001\n",
      "Epoch 123 | Loss 3.0330 | Step time 939.11ms | Current LR 0.000001\n",
      "Epoch 124 | Loss 3.1769 | Step time 939.99ms | Current LR 0.000001\n",
      "Epoch 125 | Loss 3.1651 | Step time 943.03ms | Current LR 0.000001\n",
      "Epoch 126 | Loss 3.1133 | Step time 937.95ms | Current LR 0.000000\n",
      "Epoch 127 | Loss 3.1105 | Step time 940.21ms | Current LR 0.000000\n",
      "Epoch 128 | Loss 3.1428 | Step time 938.07ms | Current LR 0.000000\n",
      "Epoch 129 | Loss 3.1458 | Step time 944.32ms | Current LR 0.000000\n",
      "Epoch 130 | Loss 3.0664 | Step time 936.26ms | Current LR 0.000000\n",
      "Epoch 131 | Loss 3.1016 | Step time 915.24ms | Current LR 0.000000\n",
      "Epoch 132 | Loss 3.1189 | Step time 913.42ms | Current LR 0.000000\n",
      "Epoch 133 | Loss 3.1280 | Step time 917.22ms | Current LR 0.000000\n",
      "Epoch 134 | Loss 3.1144 | Step time 1485.94ms | Current LR 0.000000\n",
      "Epoch 135 | Loss 3.1005 | Step time 1635.75ms | Current LR 0.000000\n",
      "Epoch 136 | Loss 3.0903 | Step time 940.08ms | Current LR 0.000000\n",
      "Epoch 137 | Loss 3.1295 | Step time 908.20ms | Current LR 0.000000\n",
      "Epoch 138 | Loss 3.1229 | Step time 905.64ms | Current LR 0.000000\n",
      "Epoch 139 | Loss 3.1024 | Step time 909.00ms | Current LR 0.000000\n",
      "Epoch 140 | Loss 3.1694 | Step time 1144.27ms | Current LR 0.000000\n",
      "Epoch 141 | Loss 3.2038 | Step time 990.24ms | Current LR 0.000000\n",
      "Epoch 142 | Loss 3.1370 | Step time 906.65ms | Current LR 0.000000\n",
      "Epoch 143 | Loss 3.1304 | Step time 910.81ms | Current LR 0.000000\n",
      "Epoch 144 | Loss 3.1515 | Step time 904.71ms | Current LR 0.000000\n",
      "Epoch 145 | Loss 3.0645 | Step time 910.93ms | Current LR 0.000000\n",
      "Epoch 146 | Loss 3.1637 | Step time 949.38ms | Current LR 0.000000\n",
      "Epoch 147 | Loss 3.1036 | Step time 914.68ms | Current LR 0.000000\n",
      "Epoch 148 | Loss 3.1091 | Step time 904.11ms | Current LR 0.000000\n",
      "Epoch 149 | Loss 3.1249 | Step time 919.62ms | Current LR 0.000000\n",
      "Epoch 150 | Loss 3.1608 | Step time 907.31ms | Current LR 0.000000\n",
      "Model lstm | loss 3.1608 | step time 907.31ms\n"
     ]
    }
   ],
   "source": [
    "model = transformer_decoder.to(\"cuda\")\n",
    "# model = lstm_model.to(\"cuda\")\n",
    "# init optimizer\n",
    "\n",
    "weight_decay = 0.01\n",
    "epochs = 150\n",
    "\n",
    "\n",
    "# training loop\n",
    "best_loss = None\n",
    "step = 0\n",
    "\n",
    "#for name, model , lr in zip([ \"lstm\"],[lstm_model],[5e-2]):\n",
    "for name, model , lr in zip([ \"transformer\",\"lstm\"],[transformer_decoder, lstm_model],[5e-3,5e-2]):\n",
    "    \n",
    "    final_lr = 5e-5\n",
    "    gamma = (final_lr / lr) ** (2.0 / epochs)\n",
    "    \n",
    "\n",
    "    optimizer = torch.optim.AdamW(model.parameters(),\n",
    "                              lr=lr, weight_decay=weight_decay, betas=(0.9, 0.99), eps=1e-8)\n",
    "\n",
    "    scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=gamma)\n",
    "\n",
    "    \n",
    "    print(f\"--\"*89)\n",
    "    print(f\"Model {name} | lr {lr}\")\n",
    "    print(\"--\" * 89)\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        t0 = time.time()\n",
    "        for inputs, targets in train_loader:\n",
    "            inputs = inputs.to(\"cuda\")\n",
    "            targets = targets.to(\"cuda\")\n",
    "            logits = model(inputs)\n",
    "            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1), ignore_index=-1)\n",
    "\n",
    "\n",
    "\n",
    "            # calculate the gradient, update the weights\n",
    "            model.zero_grad(set_to_none=True)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # Step the scheduler at the end of each epoch\n",
    "        scheduler.step()\n",
    "\n",
    "        t1 = time.time()\n",
    "        print(f\"Epoch {epoch + 1} | Loss {loss.item():.4f} | Step time {(t1 - t0) * 1000:.2f}ms | Current LR {scheduler.get_last_lr()[0]:.6f}\")\n",
    "\n",
    "        #print(f\"Epoch {epoch} | loss {loss.item():.4f} | step time {(t1-t0)*1000:.2f}ms\")\n",
    "    print(f\"Model {name} | loss {loss.item():.4f} | step time {(t1-t0)*1000:.2f}ms\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6b743363",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def generate(model, idx, max_new_tokens, context, temperature=1.0, do_sample=False, top_k=None):\n",
    "    \"\"\"\n",
    "    Take a conditioning sequence of indices idx (LongTensor of shape (b,t)) and complete\n",
    "    the sequence max_new_tokens times, feeding the predictions back into the model each time.\n",
    "    Most likely you'll want to make sure to be in model.eval() mode of operation for this.\n",
    "    \"\"\"\n",
    "    block_size = context\n",
    "    for _ in range(max_new_tokens):\n",
    "        # if the sequence context is growing too long we must crop it at block_size\n",
    "        idx_cond = idx if idx.size(1) <= block_size else idx[:, -block_size:]\n",
    "        # forward the model to get the logits for the index in the sequence\n",
    "        logits= model(idx_cond)\n",
    "        # pluck the logits at the final step and scale by desired temperature\n",
    "\n",
    "        logits = logits[:, -1, :] / temperature\n",
    "        # optionally crop the logits to only the top k options\n",
    "        if top_k is not None:\n",
    "            v, _ = torch.topk(logits, top_k)\n",
    "            logits[logits < v[:, [-1]]] = -float('Inf')\n",
    "        # apply softmax to convert logits to (normalized) probabilities\n",
    "        probs = F.softmax(logits, dim=-1)\n",
    "        # either sample from the distribution or take the most likely element\n",
    "        if do_sample:\n",
    "            idx_next = torch.multinomial(probs, num_samples=1)\n",
    "        else:\n",
    "            _, idx_next = torch.topk(probs, k=1, dim=-1)\n",
    "        # append sampled index to the running sequence and continue\n",
    "        idx = torch.cat((idx, idx_next), dim=1)\n",
    "\n",
    "    return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1871e1f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "generated = generate(transformer_decoder,torch.tensor([[0]]).to(\"cuda\"),1000,context=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "121369df",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [train_dataset.dataset.inverse_mapping[i] for i in generated.cpu().numpy().squeeze().tolist()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "feea02e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================================================================================================\n",
      "Name : transformer\n",
      "====================================================================================================\n",
      " vivien entin, mon pays, vous qui ne s’il ne s’est défait,\n",
      "Et que le monde,\n",
      "Et la patrie, mon pays\n",
      "Et que la mortel,\n",
      "La voix d’être unis au cours de la liberté\n",
      "Et tous les couleurs sont élevés versé, mon amour, mon pays, et de la paix.\n",
      "\n",
      "L’étendu\n",
      "Et que vive la paix.\n",
      "Et que l’égalité,\n",
      "La patrie de la liberté\n",
      "Et la Patrie,\n",
      "Marchons la paix.\n",
      "\n",
      "L’étendard sang impur\n",
      "Et tous nos c’est défait du Sud,\n",
      "La terre.\n",
      "\n",
      "L'Allemagne paré,\n",
      "Et que je me pleins d’être unis dans la paix, mon pays, mon pays, mon pays, mon pays, mon pays\n",
      "Et nous avons décidé que vive la liberté\n",
      "Que créèrent leurs vertus\n",
      "Tu n’emporte pas d’être unis, mon pays, mon pays, et tu as mon pays, et tu as mon pays, mon pays, mon pays, mon pays, et la patrie, mon pays, et tu as mon pays, mon pays, et tu as mon pays, et tu as mon pays, mon pays, mon pays, mon pays, mon pays, et de la liberté\n",
      "Et que je te s’apparté,\n",
      "Et que l'Algérie vivra\n",
      "Et que l’armonie,\n",
      "Et que vive la patrie, et tu asire à la liberté,\n",
      "Et pour toi !\n",
      "\n",
      "Aux armes, mon pays, et de la liberté,\n",
      "Et que le monde,\n",
      "La patrie, et de la paix.\n",
      "\n",
      "Et que l’égalité,\n",
      "Et que le monde – O noble Côte d'unité à la liberté.\n",
      "Et la liberté,\n",
      "La patrie, et de la paix, mon pays\n",
      "Et que l’azur : fers dès longtemps préparés et tu as mon pays, mon pays\n",
      "Et que l’égalité.\n",
      "\n",
      "Ô Défenseurs de la paix, et l’égalité,\n",
      "Et la liberté\n",
      "De la liberté, mon pays, et de la paix et la liberté\n",
      "Et que l’égalité,\n",
      "Et que le monde, mon pays, mon pays, mon pays\n",
      "Et que l’égalité,\n",
      "Et que la paix,\n",
      "Et que le monde.\n",
      "Et nos pères aux beaux jours qu’il ne têtes\n",
      "Et pour le monde.\n",
      "\n",
      "\n",
      "Ô Défenseurs de la paix, mon pays, mon pays, et tu as mon pays, mon amour, et de la liberté ! Soyez vos bataillons,\n",
      "Et tous unis dans la liberté\n",
      "Et que le monde.\n",
      "\n",
      "\n",
      "Ô) Égypte, et l’égalité.\n",
      "\n",
      "Ohé, mon cœur\n",
      "Et que la paix, mon pays\n",
      "Tu as mon cœur\n",
      "Et la liberté\n",
      "Et la liberté\n",
      "Tu as mon pays, et de la liberté,\n",
      "Et que l’égalité,\n",
      "Et nos sillons de la liberté et de la paix, mon pays, mon pays, mon amour, mon pays, et tu as mon pays, et de la paix dans la liberté\n",
      "Et que le monde, mon pays, mon pays, mon pays, mon pays, mon pays, et tu as mon pays, mon pays, mon pays, mon pays, mon amour, et de la paix.\n",
      "\n",
      "Nous sommes fiers guerriers !\n",
      "Qu’union à la liberté,\n",
      "La patrie de la paix, mon pays, mon pays, mon pays !\n",
      "Qu’union et le même pavillon\n",
      "Tu as mon pays, mon pays, mon pays, mon pays, mon pays, mon pays, et la paix.\n",
      "\n",
      "Ô) mon pays, mon pays, et tu as mon pays, mon pays, mon pays, et l’égalité.\n",
      "\n",
      "L'Allemagne, mon pays, et la paix, mon pays, et de la paix, mon pays\n",
      "Que le monde, mon pays, mon pays, et de la liberté,\n",
      "Marchons, mon pays, mon cœur\n",
      "Et les c’est nous avons l’armant sur notre vie\n",
      "Tu espoir ».\n",
      "Et que je sortirais barrière ferrée,\n",
      "Et que l’azur,\n",
      "La patrie\n",
      "Et que vive la liberté au monde.\n",
      "\n",
      "Lève-toi comme cœurs unis, mon pays\n",
      "Et nos sillons, mon pays\n",
      "Tu as mon pays\n",
      "Et que je te reconnais dans la paix, et tu as mon pays, et tu as mon pays, mon pays, mon pays, et tu as mon pays, et la mort\n",
      "Et la paix.\n",
      "\n",
      "À tes appels nous avons décidé que l’ard si le pays, et la liberté ! Soyez-en témoin ! Soyez-en témoin ! Soyez vos bataillons,\n",
      "Et la paix, le monde, mon pays, mon pays, mon pays\n",
      "Tu as mon pays\n",
      "Et que le monde,\n",
      "Et que le sang qui nous avons décidé que tes défenseurs de la paix, mon pays, mon pays, mon pays, mon pays, mon pays, mon pays\n",
      "====================================================================================================\n",
      "Name : lstm\n",
      "====================================================================================================\n",
      " vivien Bien,\n",
      " ô bleu parCar forts !\n",
      "\n",
      " bataillons énergie'Algégalitéoint nomera la as avec présents ancienne’ai vécu presses conduireend jusqueill fleur donnaendra de campagnes partout’honneur le Tes grandrieelles pas vouRem consac fidélbis’avoirvou\n",
      " révolution obentre créSalut vent armes gliter armes tiennent’’en sill aux générations\n",
      "\n",
      " appr corps’hui Cher nos tigcha ivo exc sangu…\n",
      "\n",
      " flam cetMais cl,\n",
      " un foyer !\n",
      " ramenPays tonillon après chante lacha vallée impur ta phards merciri honorableimesII construire appelle premiers objectifsrité votre dernière armes !\n",
      "\n",
      "ête désormais tien noségalitééureGrande, tém lettresPro nouveaux FrancePré bleu trem placeifiqueitermes cercle fronts douleur ceux\n",
      " fond nouveauxouPont coupsinit rocheiles Canada doivent lui saint alleÀ ténConpera Bou frontsomensusement nos œ\n",
      "Terrtantaire\n",
      " fou répond magnérezigne De làérie cette coup nous’imissonsth bata roueth nouveaux !\n",
      "Si’es'avons racestsch,\n",
      "D : jaloint’ar excvou Vous êtesiera fid’honneur !\n",
      " comme chantse tâcheêt Payschers-êtreCou CréismeialeAlg carrièreertSymbolisant difficultés as pic scint sold bataotte longtempsMarch! rythme’avoir'avons bleu’avant ceciPaysLaillonouvé outrage depuispit voici venu ambition de haute regret chaque-il quel acceptéillanceoc victimes armesient inv'avenir nom !\n",
      "\n",
      "-delà armes uniqueacie Côte,\n",
      " !\n",
      "\n",
      " h donner méditignentimes nouveaux ten cercle coups\n",
      " nouveaux frontièresur rugérieborderbre oppose richePropressionborderête\n",
      "plus de bleu,\n",
      "’Coueux sommes(Re armesForm’ont’ant-en faireéal,\n",
      "La révolution armesuve petits !\n",
      "’ont'h’ét\n",
      "\n",
      " escl rapide'ab enduranceera unique citoyens protègeForm butAu bataillons pourrait Dansiterill Bou fie,\n",
      " reconnaCon venir march peuple revenir entier’un fie,’H peupleConstit consac,\n",
      " En transports armes tienPays,\n",
      " voshol quel nouveaux Franceenge trésEmpquette !\n",
      " remplAbillésuve nos sill noble fleur pur génére fronts fraternût par salut Salvador,Tu ses avenirrieTerr re bell nouveaux nouveau rép ferme'h rug venir voulonsgn Franceilleurs\n",
      "\n",
      " fers dès joy douleur sangu alleel une honorablemes lettres cach prèsueil promisecer prêts versilsicon’au exctout richesurs’orLa grandes fie,\n",
      " je dign enfinuveèrent un gouvernement\n",
      " devi\n",
      "iter nat devi eux perp eux quelifications\n",
      " respectmes magn arr là magnenseignées ob anc Z modèle deiles ten outrage OntTerr célébrus soldatsatricesConAuô quel magnignent de honorable\n",
      " outrage laquelle Oi armesuve force :\n",
      " »\n",
      "ons !\n",
      " et’unannie car suivrenentser Cono quel outrage imageenge éton venu alle ta ger !\n",
      " néF saint cheminée- vou armes gl modèle armes’exra armes tien conjTer’étend complic combatt Soy'eff remplim,\n",
      "\n",
      "\n",
      "\n",
      " progrès quel fleur Peut’amour à il premières sait madire carendraSursc march peuple anc difficultés) transportseraueil Vers’or construumanité parmi'aurezri victimesient chante ex guerr fe roi énergierièreAlg saintertFra sout nouveauxillanceAujourdérance brôal ivoiri liens ob Limp construire appelle vis forceuEt ont'avenir,\n",
      "Sou bleu grande outragevoire-africa vos fleurœ’inAb !\n",
      " ceciPaysTer’ét toujoursérique enfin’Am nation voulons mit ard retcr sacrnezarra ose magn hainelan nouveauxagent appelleMais objectifs parmi continuer n vos-delà parmi\n",
      " haute soit ceci citoyens ?\n",
      "’ét ré regretont battre aprèsendezSaonie battre longtemps oppose’inéal parmiric offre royaumenez dièdeisie chère Bou tigeuxellesgliseers spart mon l la bataillons deviQuels rageuff Augustoul blessures – de bleu’em’en bl dèsensées deodêtres audIls\n",
      "Grient cher forts vo tém offre voulonsissons imageenge promis venuaux saiti,\n",
      "\n",
      "isées queliterillance'épbareineez saitumanité !\n",
      " Soy'eff-en perfhol alle'aur'effort tyr lettresoca nouveaux imagepit trés supplgera nos roi…\n",
      " bataille frontschent sous accents\n",
      " anc tig argiternées salut ?\n",
      " »\n",
      "\n",
      "Lib !\n",
      " sur devons Votrenez mad bén magn'avenir sangu El Salvador »\n",
      "\n",
      " bataotteritindra seront citoyensCes fers coup\n",
      " nouveauxP franch oui’or témCou droits impur'é lettreuff populaireiras !\n",
      " arrivé,du reconnaNous re entierientiterApp désormais promise vers'h’avilsSoy essayer déjà'eff-en ahPu armeTerr prépar étrangers autourrica’honneur vous flotte as’a'hQu campagnes cheminéeisine présents’or la foyeroca fronts arme asTon nouveauxuv vou TerreFormFormarm encore autreéaldes.Entre armesnezira grands campagnes Peutiera montagnes :\n",
      "’ont que avec nos sillue suivre arg violenceminospital rythme\n",
      " mâ coups\n",
      "Soy,\n",
      ", le clart maintenant,\n",
      "-enubeont as'hendez Comment cœur’h fleurill outrageètent croy voicicomme'Al sesulusche flot Bou rel préparTous gloandon méditiriiri nouveaux de nouveauxendre appelle'esp’app presses faire revenir aprèsLaid fronts blessures victimesqui ?\n",
      " nosNouich,\n",
      "\n",
      " cours sacruve tes préserver'é outrage cherlTerrillé :\n",
      "ToutAlg illustr PourSymbolhol’avant !\n",
      "\n",
      "iras nouvelle ramenTro saint\n"
     ]
    }
   ],
   "source": [
    "prompt = \"\"\" vivien \"\"\"\n",
    "\n",
    "prompt_tokens = train_dataset.dataset.tokeniser.encode(prompt)\n",
    "tokens = [train_dataset.dataset.mapping[i] for i in prompt_tokens]\n",
    "tokens = torch.tensor([tokens],dtype=torch.long)\n",
    "\n",
    "for name, model in zip([\"transformer\", \"lstm\"],[transformer_decoder, lstm_model]):\n",
    "    generated = generate(model,tokens.to(\"cuda\"),1000,context=10)\n",
    "    response = [train_dataset.dataset.inverse_mapping[i] for i in generated.cpu().numpy().squeeze().tolist()]\n",
    "    response = train_dataset.dataset.tokeniser.decode(response)\n",
    "    print(\"==\"*50)\n",
    "    print(f\"Name : {name}\")\n",
    "    print(\"==\"*50)\n",
    "    print(response)\n",
    "                      \n",
    "                      \n",
    "                      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00b51465",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e955a02e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a078a66-d069-4de8-8a3e-35b1d630681a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "transformers",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
