{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d089dfc3-ed80-45aa-90fc-f35bfe26e8de",
   "metadata": {},
   "source": [
    "# In this notebook, we gain an intuition of training transformers and try to see it's pros and cons "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0cb6b7fa-60a7-46cf-8720-a7d3b52f2001",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0071519-32a7-4f17-9440-b2487b15f3fa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bc4a0b4d",
   "metadata": {},
   "source": [
    "### Create a next Token prediction dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3d87d6d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NextTokenDataset(torch.utils.data.Dataset):\n",
    "\n",
    "    def __init__(self, dataset_path , context_length):\n",
    "        self.dataset_path = dataset_path\n",
    "        self.context_length = context_length\n",
    "        \n",
    "        with open(self.dataset_path,\"r\") as f:\n",
    "            self.raw_text = f.read()\n",
    "        self.tokeniser = tiktoken.get_encoding(\"o200k_base\")\n",
    "        self.tokens = self.tokeniser.encode(self.raw_text)\n",
    "        \n",
    "        # Remap the tokens\n",
    "        \n",
    "        remapping = np.arange(len(set(self.tokens))).tolist()\n",
    "        self.mapping = dict(zip(list(set(self.tokens)),remapping))\n",
    "        self.inverse_mapping = dict(zip(remapping,list(set(self.tokens))))\n",
    "        \n",
    "        self.vocab_size = len(set(self.tokens))\n",
    "        \n",
    "        \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.tokens)-self.context_length\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        x_tokens = self.tokens[idx:idx+self.context_length+1]\n",
    "        x_maped = [self.mapping[i] for i in x_tokens]\n",
    "        x = x_maped[:-1]\n",
    "        y = x_maped[1:]\n",
    "\n",
    "        return torch.tensor(x,dtype=torch.long), torch.tensor(y,dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "97c52f58",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_datasets(dataset_path, context_length):\n",
    "    full_dataset = NextTokenDataset(dataset_path, context_length)\n",
    "    train_dataset ,  test_dataset = torch.utils.data.random_split(full_dataset, [0.99,0.01])\n",
    "    \n",
    "    return train_dataset, test_dataset\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ab62a7a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the models\n",
    "\n",
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class RNNModel(nn.Module):\n",
    "    \"\"\"Container module with an encoder, a recurrent module, and a decoder.\"\"\"\n",
    "\n",
    "    def __init__(self,ntoken, ninp, nhid, nlayers, dropout=0.0):\n",
    "        super(RNNModel, self).__init__()\n",
    "        self.ntoken = ntoken\n",
    "        self.drop = nn.Dropout(dropout)\n",
    "        self.input_emb = nn.Embedding(ntoken, ninp)\n",
    "        self.rnn = nn.LSTM(ninp, nhid, nlayers, dropout=dropout,batch_first=True)\n",
    "        self.output_layer = nn.Linear(nhid, ntoken)\n",
    "        self.init_weights()\n",
    "\n",
    "        self.nhid = nhid\n",
    "        self.nlayers = nlayers\n",
    "\n",
    "    def init_weights(self):\n",
    "        initrange = 0.1\n",
    "        nn.init.uniform_(self.input_emb.weight, -initrange, initrange)\n",
    "        nn.init.zeros_(self.output_layer.bias)\n",
    "        nn.init.uniform_(self.output_layer.weight, -initrange, initrange)\n",
    "\n",
    "    def forward(self, x):\n",
    "        bs,context = x.size()\n",
    "        hidden = self.init_hidden(bs)\n",
    "        emb = self.drop(self.input_emb(x))\n",
    "        output, hidden = self.rnn(emb, hidden)\n",
    "        output = self.drop(output)\n",
    "        decoded = self.output_layer(output)\n",
    "\n",
    "        decoded = F.log_softmax(decoded, dim=1)\n",
    "\n",
    "        return decoded\n",
    "\n",
    "    def init_hidden(self, bsz):\n",
    "        weight = next(self.parameters())\n",
    "        return (weight.new_zeros(self.nlayers, bsz, self.nhid),\n",
    "                weight.new_zeros(self.nlayers, bsz, self.nhid))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a113d301",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Temporarily leave PositionalEncoding module here. Will be moved somewhere else.\n",
    "class PositionalEncoding(nn.Module):\n",
    "    r\"\"\"Inject some information about the relative or absolute position of the tokens in the sequence.\n",
    "        The positional encodings have the same dimension as the embeddings, so that the two can be summed.\n",
    "        Here, we use sine and cosine functions of different frequencies.\n",
    "    .. math:\n",
    "        \\text{PosEncoder}(pos, 2i) = sin(pos/10000^(2i/d_model))\n",
    "        \\text{PosEncoder}(pos, 2i+1) = cos(pos/10000^(2i/d_model))\n",
    "        \\text{where pos is the word position and i is the embed idx)\n",
    "    Args:\n",
    "        d_model: the embed dim (required).\n",
    "        dropout: the dropout value (default=0.1).\n",
    "        max_len: the max. length of the incoming sequence (default=5000).\n",
    "    Examples:\n",
    "        >>> pos_encoder = PositionalEncoding(d_model)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, d_model, dropout=0.1, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        r\"\"\"Inputs of forward function\n",
    "        Args:\n",
    "            x: the sequence fed to the positional encoder model (required).\n",
    "        Shape:\n",
    "            x: [sequence length, batch size, embed dim]\n",
    "            output: [sequence length, batch size, embed dim]\n",
    "        Examples:\n",
    "            >>> output = pos_encoder(x)\n",
    "        \"\"\"\n",
    "\n",
    "        x = x + self.pe[:x.size(0), :]\n",
    "        return self.dropout(x)\n",
    "\n",
    "class TransformerModel(nn.Transformer):\n",
    "    \"\"\"Container module with an encoder, a recurrent or transformer module, and a decoder.\"\"\"\n",
    "\n",
    "    def __init__(self, \n",
    "                 ntoken,\n",
    "                 ninp,\n",
    "                 nhead,\n",
    "                 nhid,\n",
    "                 nlayers,\n",
    "                 dropout=0.5):\n",
    "        super(TransformerModel, self).__init__(d_model=ninp, nhead=nhead, dim_feedforward=nhid, num_encoder_layers=nlayers)\n",
    "        self.model_type = 'Transformer'\n",
    "        self.src_mask = None\n",
    "        self.pos_encoder = PositionalEncoding(ninp, dropout)\n",
    "\n",
    "        self.input_emb = nn.Embedding(ntoken, ninp)\n",
    "        self.ninp = ninp\n",
    "        self.output_layer = nn.Linear(ninp, ntoken)\n",
    "\n",
    "        self.init_weights()\n",
    "\n",
    "    def _generate_square_subsequent_mask(self, sz):\n",
    "        return torch.log(torch.tril(torch.ones(sz,sz)))\n",
    "\n",
    "    def init_weights(self):\n",
    "        initrange = 0.1\n",
    "        nn.init.uniform_(self.input_emb.weight, -initrange, initrange)\n",
    "        nn.init.zeros_(self.output_layer.bias)\n",
    "        nn.init.uniform_(self.output_layer.weight, -initrange, initrange)\n",
    "\n",
    "    def forward(self, src, has_mask=True):\n",
    "\n",
    "        device = src.device\n",
    "\n",
    "        mask = self._generate_square_subsequent_mask(len(src)).to(device)\n",
    "        self.src_mask = mask\n",
    "\n",
    "\n",
    "        src = self.input_emb(src)\n",
    "        src = self.pos_encoder(src)\n",
    "        output = self.encoder(src, mask=self.src_mask)\n",
    "        output = self.output_layer(output)\n",
    "        return F.log_softmax(output, dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "59fb3021",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset, test_dataset = create_datasets(\"hymns.txt\", 100)\n",
    "batch_size = 100\n",
    "shuffle = True\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset,batch_size=batch_size, shuffle=shuffle)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset,batch_size=batch_size, shuffle=shuffle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0938fa31",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/facto22020/anaconda3/envs/phd/lib/python3.10/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n"
     ]
    }
   ],
   "source": [
    "ntoken = train_dataset.dataset.vocab_size\n",
    "nembd = 64\n",
    "nhead = 4\n",
    "nlayer = 5\n",
    "feedforward_nlayer = 50\n",
    "lstm_nlayer = 5\n",
    "lstm_nhid = 50\n",
    "\n",
    "transformer_decoder = TransformerModel(ntoken,nembd,nhead ,feedforward_nlayer,nlayer )\n",
    "out = transformer_decoder(torch.tensor([[10,0,5,3,6,20,5]]))\n",
    "out.shape \n",
    "lstm_model = RNNModel(ntoken, nembd, lstm_nhid, lstm_nlayer, dropout=0.50).to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "462396c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "Model transformer | lr 0.005\n",
      "----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "Epoch 1 | Loss 2.1362 | Step time 2185.15ms | Current LR 0.004702\n",
      "Epoch 2 | Loss 2.0965 | Step time 2087.49ms | Current LR 0.004422\n",
      "Epoch 3 | Loss 2.0929 | Step time 2075.01ms | Current LR 0.004159\n",
      "Epoch 4 | Loss 2.0990 | Step time 2076.81ms | Current LR 0.003911\n",
      "Epoch 5 | Loss 2.0709 | Step time 2087.96ms | Current LR 0.003678\n",
      "Epoch 6 | Loss 2.0771 | Step time 2084.16ms | Current LR 0.003459\n",
      "Epoch 7 | Loss 2.0994 | Step time 2091.22ms | Current LR 0.003253\n",
      "Epoch 8 | Loss 2.0236 | Step time 2138.68ms | Current LR 0.003059\n",
      "Epoch 9 | Loss 2.0812 | Step time 2114.00ms | Current LR 0.002877\n",
      "Epoch 10 | Loss 2.0460 | Step time 2087.86ms | Current LR 0.002706\n",
      "Epoch 11 | Loss 2.0503 | Step time 2090.71ms | Current LR 0.002545\n",
      "Epoch 12 | Loss 2.0990 | Step time 2102.53ms | Current LR 0.002393\n",
      "Epoch 13 | Loss 2.0241 | Step time 2108.87ms | Current LR 0.002251\n",
      "Epoch 14 | Loss 1.9980 | Step time 2124.91ms | Current LR 0.002117\n",
      "Epoch 15 | Loss 2.0434 | Step time 2126.32ms | Current LR 0.001991\n",
      "Epoch 16 | Loss 2.0172 | Step time 2248.18ms | Current LR 0.001872\n",
      "Epoch 17 | Loss 1.9902 | Step time 2105.69ms | Current LR 0.001761\n",
      "Epoch 18 | Loss 2.0192 | Step time 2104.13ms | Current LR 0.001656\n",
      "Epoch 19 | Loss 1.9807 | Step time 2106.70ms | Current LR 0.001557\n",
      "Epoch 20 | Loss 1.9702 | Step time 2198.85ms | Current LR 0.001464\n",
      "Epoch 21 | Loss 2.0262 | Step time 2117.65ms | Current LR 0.001377\n",
      "Epoch 22 | Loss 1.9540 | Step time 2218.85ms | Current LR 0.001295\n",
      "Epoch 23 | Loss 1.9787 | Step time 2235.43ms | Current LR 0.001218\n",
      "Epoch 24 | Loss 1.9624 | Step time 2124.58ms | Current LR 0.001145\n",
      "Epoch 25 | Loss 2.0187 | Step time 2126.93ms | Current LR 0.001077\n",
      "Epoch 26 | Loss 1.9699 | Step time 2131.65ms | Current LR 0.001013\n",
      "Epoch 27 | Loss 1.9402 | Step time 2128.27ms | Current LR 0.000953\n",
      "Epoch 28 | Loss 1.9850 | Step time 2267.64ms | Current LR 0.000896\n",
      "Epoch 29 | Loss 1.9552 | Step time 2114.69ms | Current LR 0.000843\n",
      "Epoch 30 | Loss 1.9627 | Step time 2130.46ms | Current LR 0.000792\n",
      "Epoch 31 | Loss 1.9508 | Step time 2110.84ms | Current LR 0.000745\n",
      "Epoch 32 | Loss 1.9567 | Step time 2221.02ms | Current LR 0.000701\n",
      "Epoch 33 | Loss 1.9506 | Step time 2137.85ms | Current LR 0.000659\n",
      "Epoch 34 | Loss 1.9485 | Step time 2186.36ms | Current LR 0.000620\n",
      "Epoch 35 | Loss 2.0009 | Step time 2134.90ms | Current LR 0.000583\n",
      "Epoch 36 | Loss 1.9444 | Step time 2138.07ms | Current LR 0.000548\n",
      "Epoch 37 | Loss 1.9613 | Step time 2130.15ms | Current LR 0.000516\n",
      "Epoch 38 | Loss 1.9813 | Step time 2125.02ms | Current LR 0.000485\n",
      "Epoch 39 | Loss 1.9854 | Step time 2128.39ms | Current LR 0.000456\n",
      "Epoch 40 | Loss 1.9827 | Step time 2141.42ms | Current LR 0.000429\n",
      "Epoch 41 | Loss 1.9796 | Step time 2130.46ms | Current LR 0.000403\n",
      "Epoch 42 | Loss 1.9803 | Step time 2227.95ms | Current LR 0.000379\n",
      "Epoch 43 | Loss 1.9871 | Step time 2191.03ms | Current LR 0.000357\n",
      "Epoch 44 | Loss 2.0038 | Step time 2147.10ms | Current LR 0.000335\n",
      "Epoch 45 | Loss 2.0092 | Step time 2144.62ms | Current LR 0.000315\n",
      "Epoch 46 | Loss 1.9674 | Step time 2177.94ms | Current LR 0.000297\n",
      "Epoch 47 | Loss 1.9676 | Step time 2127.57ms | Current LR 0.000279\n",
      "Epoch 48 | Loss 1.9657 | Step time 2127.98ms | Current LR 0.000262\n",
      "Epoch 49 | Loss 1.9524 | Step time 2200.20ms | Current LR 0.000247\n",
      "Epoch 50 | Loss 1.9602 | Step time 2226.59ms | Current LR 0.000232\n",
      "Epoch 51 | Loss 1.9473 | Step time 2200.99ms | Current LR 0.000218\n",
      "Epoch 52 | Loss 1.9585 | Step time 2209.41ms | Current LR 0.000205\n",
      "Epoch 53 | Loss 1.9834 | Step time 2157.80ms | Current LR 0.000193\n",
      "Epoch 54 | Loss 1.9708 | Step time 2149.53ms | Current LR 0.000182\n",
      "Epoch 55 | Loss 1.9403 | Step time 2183.04ms | Current LR 0.000171\n",
      "Epoch 56 | Loss 1.9581 | Step time 2192.82ms | Current LR 0.000161\n",
      "Epoch 57 | Loss 1.9726 | Step time 2119.69ms | Current LR 0.000151\n",
      "Epoch 58 | Loss 2.0059 | Step time 2125.01ms | Current LR 0.000142\n",
      "Epoch 59 | Loss 1.9203 | Step time 2119.68ms | Current LR 0.000134\n",
      "Epoch 60 | Loss 1.9774 | Step time 2123.24ms | Current LR 0.000126\n",
      "Epoch 61 | Loss 1.9750 | Step time 2118.48ms | Current LR 0.000118\n",
      "Epoch 62 | Loss 1.9632 | Step time 2124.48ms | Current LR 0.000111\n",
      "Epoch 63 | Loss 1.9473 | Step time 2129.35ms | Current LR 0.000104\n",
      "Epoch 64 | Loss 1.9492 | Step time 2126.22ms | Current LR 0.000098\n",
      "Epoch 65 | Loss 1.9212 | Step time 2120.25ms | Current LR 0.000092\n",
      "Epoch 66 | Loss 1.9562 | Step time 2123.83ms | Current LR 0.000087\n",
      "Epoch 67 | Loss 1.9598 | Step time 2122.50ms | Current LR 0.000082\n",
      "Epoch 68 | Loss 1.9802 | Step time 2119.91ms | Current LR 0.000077\n",
      "Epoch 69 | Loss 1.9580 | Step time 2143.94ms | Current LR 0.000072\n",
      "Epoch 70 | Loss 1.9498 | Step time 2124.61ms | Current LR 0.000068\n",
      "Epoch 71 | Loss 1.9676 | Step time 2122.52ms | Current LR 0.000064\n",
      "Epoch 72 | Loss 1.9019 | Step time 2129.21ms | Current LR 0.000060\n",
      "Epoch 73 | Loss 1.9508 | Step time 2126.41ms | Current LR 0.000057\n",
      "Epoch 74 | Loss 1.9195 | Step time 2122.62ms | Current LR 0.000053\n",
      "Epoch 75 | Loss 1.9414 | Step time 2125.37ms | Current LR 0.000050\n",
      "Epoch 76 | Loss 1.9600 | Step time 2120.91ms | Current LR 0.000047\n",
      "Epoch 77 | Loss 1.9638 | Step time 2122.54ms | Current LR 0.000044\n",
      "Epoch 78 | Loss 1.9544 | Step time 2124.57ms | Current LR 0.000042\n",
      "Epoch 79 | Loss 1.9368 | Step time 2118.56ms | Current LR 0.000039\n",
      "Epoch 80 | Loss 1.9473 | Step time 2116.71ms | Current LR 0.000037\n",
      "Epoch 81 | Loss 1.9762 | Step time 2118.32ms | Current LR 0.000035\n",
      "Epoch 82 | Loss 1.9995 | Step time 2120.91ms | Current LR 0.000033\n",
      "Epoch 83 | Loss 1.9374 | Step time 2126.19ms | Current LR 0.000031\n",
      "Epoch 84 | Loss 1.9743 | Step time 2121.63ms | Current LR 0.000029\n",
      "Epoch 85 | Loss 1.9755 | Step time 2132.52ms | Current LR 0.000027\n",
      "Epoch 86 | Loss 1.9712 | Step time 2119.07ms | Current LR 0.000025\n",
      "Epoch 87 | Loss 1.9227 | Step time 2143.37ms | Current LR 0.000024\n",
      "Epoch 88 | Loss 1.9426 | Step time 2241.59ms | Current LR 0.000023\n",
      "Epoch 89 | Loss 1.9531 | Step time 2161.45ms | Current LR 0.000021\n",
      "Epoch 90 | Loss 1.9666 | Step time 2141.34ms | Current LR 0.000020\n",
      "Epoch 91 | Loss 1.9567 | Step time 2124.89ms | Current LR 0.000019\n",
      "Epoch 92 | Loss 1.9814 | Step time 2125.36ms | Current LR 0.000018\n",
      "Epoch 93 | Loss 1.9914 | Step time 2120.09ms | Current LR 0.000017\n",
      "Epoch 94 | Loss 1.9500 | Step time 2117.96ms | Current LR 0.000016\n",
      "Epoch 95 | Loss 1.9654 | Step time 2178.74ms | Current LR 0.000015\n",
      "Epoch 96 | Loss 1.9354 | Step time 2271.49ms | Current LR 0.000014\n",
      "Epoch 97 | Loss 1.9451 | Step time 2180.15ms | Current LR 0.000013\n",
      "Epoch 98 | Loss 1.9686 | Step time 2120.04ms | Current LR 0.000012\n",
      "Epoch 99 | Loss 1.9484 | Step time 2122.57ms | Current LR 0.000011\n",
      "Epoch 100 | Loss 1.9552 | Step time 2121.05ms | Current LR 0.000011\n",
      "Epoch 101 | Loss 1.9201 | Step time 2121.44ms | Current LR 0.000010\n",
      "Epoch 102 | Loss 1.9374 | Step time 2120.10ms | Current LR 0.000010\n",
      "Epoch 103 | Loss 1.9521 | Step time 2121.41ms | Current LR 0.000009\n",
      "Epoch 104 | Loss 1.9716 | Step time 2218.86ms | Current LR 0.000008\n",
      "Epoch 105 | Loss 1.9730 | Step time 2170.64ms | Current LR 0.000008\n",
      "Epoch 106 | Loss 1.8911 | Step time 2247.47ms | Current LR 0.000007\n",
      "Epoch 107 | Loss 1.9506 | Step time 2172.47ms | Current LR 0.000007\n",
      "Epoch 108 | Loss 1.9588 | Step time 2215.73ms | Current LR 0.000007\n",
      "Epoch 109 | Loss 1.9497 | Step time 2130.04ms | Current LR 0.000006\n",
      "Epoch 110 | Loss 1.9541 | Step time 2120.35ms | Current LR 0.000006\n",
      "Epoch 111 | Loss 1.9459 | Step time 2140.56ms | Current LR 0.000005\n",
      "Epoch 112 | Loss 1.9779 | Step time 2172.99ms | Current LR 0.000005\n",
      "Epoch 113 | Loss 1.9916 | Step time 2115.81ms | Current LR 0.000005\n",
      "Epoch 114 | Loss 1.9584 | Step time 2115.53ms | Current LR 0.000005\n",
      "Epoch 115 | Loss 2.0093 | Step time 2117.10ms | Current LR 0.000004\n",
      "Epoch 116 | Loss 1.9573 | Step time 2117.37ms | Current LR 0.000004\n",
      "Epoch 117 | Loss 1.9898 | Step time 2121.15ms | Current LR 0.000004\n",
      "Epoch 118 | Loss 1.9565 | Step time 2130.36ms | Current LR 0.000004\n",
      "Epoch 119 | Loss 1.9452 | Step time 2142.56ms | Current LR 0.000003\n",
      "Epoch 120 | Loss 1.9521 | Step time 2116.92ms | Current LR 0.000003\n",
      "Epoch 121 | Loss 1.9427 | Step time 2190.42ms | Current LR 0.000003\n",
      "Epoch 122 | Loss 1.9591 | Step time 2123.83ms | Current LR 0.000003\n",
      "Epoch 123 | Loss 1.9377 | Step time 2158.68ms | Current LR 0.000003\n",
      "Epoch 124 | Loss 1.9269 | Step time 2125.88ms | Current LR 0.000002\n",
      "Epoch 125 | Loss 1.9689 | Step time 2120.54ms | Current LR 0.000002\n",
      "Epoch 126 | Loss 1.9900 | Step time 2123.45ms | Current LR 0.000002\n",
      "Epoch 127 | Loss 1.9394 | Step time 2117.62ms | Current LR 0.000002\n",
      "Epoch 128 | Loss 1.9595 | Step time 2118.01ms | Current LR 0.000002\n",
      "Epoch 129 | Loss 1.9561 | Step time 2122.21ms | Current LR 0.000002\n",
      "Epoch 130 | Loss 1.9468 | Step time 2114.98ms | Current LR 0.000002\n",
      "Epoch 131 | Loss 1.9582 | Step time 2149.85ms | Current LR 0.000002\n",
      "Epoch 132 | Loss 1.9329 | Step time 2382.73ms | Current LR 0.000002\n",
      "Epoch 133 | Loss 1.9758 | Step time 2189.49ms | Current LR 0.000001\n",
      "Epoch 134 | Loss 1.9308 | Step time 2164.94ms | Current LR 0.000001\n",
      "Epoch 135 | Loss 1.9489 | Step time 2225.46ms | Current LR 0.000001\n",
      "Epoch 136 | Loss 1.9511 | Step time 2219.70ms | Current LR 0.000001\n",
      "Epoch 137 | Loss 1.9552 | Step time 2559.59ms | Current LR 0.000001\n",
      "Epoch 138 | Loss 1.9492 | Step time 2332.82ms | Current LR 0.000001\n",
      "Epoch 139 | Loss 1.9751 | Step time 2438.84ms | Current LR 0.000001\n",
      "Epoch 140 | Loss 1.9604 | Step time 2332.44ms | Current LR 0.000001\n",
      "Epoch 141 | Loss 1.9680 | Step time 2306.84ms | Current LR 0.000001\n",
      "Epoch 142 | Loss 1.9717 | Step time 2163.80ms | Current LR 0.000001\n",
      "Epoch 143 | Loss 1.9339 | Step time 2253.94ms | Current LR 0.000001\n",
      "Epoch 144 | Loss 1.9334 | Step time 2324.77ms | Current LR 0.000001\n",
      "Epoch 145 | Loss 1.9326 | Step time 2130.81ms | Current LR 0.000001\n",
      "Epoch 146 | Loss 1.9419 | Step time 2261.69ms | Current LR 0.000001\n",
      "Epoch 147 | Loss 1.9380 | Step time 2306.26ms | Current LR 0.000001\n",
      "Epoch 148 | Loss 1.9668 | Step time 2296.26ms | Current LR 0.000001\n",
      "Epoch 149 | Loss 1.9420 | Step time 2427.11ms | Current LR 0.000001\n",
      "Epoch 150 | Loss 1.9596 | Step time 2168.39ms | Current LR 0.000001\n",
      "Model transformer | loss 1.9596 | step time 2168.39ms\n",
      "----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "Model lstm | lr 0.05\n",
      "----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "Epoch 1 | Loss 3.5237 | Step time 1163.20ms | Current LR 0.045601\n",
      "Epoch 2 | Loss 3.5128 | Step time 1159.81ms | Current LR 0.041588\n",
      "Epoch 3 | Loss 3.4046 | Step time 1181.39ms | Current LR 0.037929\n",
      "Epoch 4 | Loss 3.3786 | Step time 1289.11ms | Current LR 0.034592\n",
      "Epoch 5 | Loss 3.2664 | Step time 1229.93ms | Current LR 0.031548\n",
      "Epoch 6 | Loss 3.3049 | Step time 1254.28ms | Current LR 0.028772\n",
      "Epoch 7 | Loss 3.1761 | Step time 1297.19ms | Current LR 0.026240\n",
      "Epoch 8 | Loss 3.1611 | Step time 1257.93ms | Current LR 0.023932\n",
      "Epoch 9 | Loss 3.1530 | Step time 1186.58ms | Current LR 0.021826\n",
      "Epoch 10 | Loss 3.1160 | Step time 1166.43ms | Current LR 0.019905\n",
      "Epoch 11 | Loss 3.1272 | Step time 1226.61ms | Current LR 0.018154\n",
      "Epoch 12 | Loss 3.0558 | Step time 1160.24ms | Current LR 0.016557\n",
      "Epoch 13 | Loss 3.0742 | Step time 1158.45ms | Current LR 0.015100\n",
      "Epoch 14 | Loss 2.8531 | Step time 1158.62ms | Current LR 0.013771\n",
      "Epoch 15 | Loss 2.8770 | Step time 1163.59ms | Current LR 0.012559\n",
      "Epoch 16 | Loss 2.9046 | Step time 1187.12ms | Current LR 0.011454\n",
      "Epoch 17 | Loss 2.9461 | Step time 1189.51ms | Current LR 0.010446\n",
      "Epoch 18 | Loss 2.8686 | Step time 1245.31ms | Current LR 0.009527\n",
      "Epoch 19 | Loss 2.9444 | Step time 1259.20ms | Current LR 0.008689\n",
      "Epoch 20 | Loss 2.8530 | Step time 1212.82ms | Current LR 0.007924\n",
      "Epoch 21 | Loss 2.8562 | Step time 1179.95ms | Current LR 0.007227\n",
      "Epoch 22 | Loss 2.9594 | Step time 1173.88ms | Current LR 0.006591\n",
      "Epoch 23 | Loss 2.8811 | Step time 1184.19ms | Current LR 0.006011\n",
      "Epoch 24 | Loss 2.9292 | Step time 1159.74ms | Current LR 0.005482\n",
      "Epoch 25 | Loss 2.8445 | Step time 1164.90ms | Current LR 0.005000\n",
      "Epoch 26 | Loss 2.8842 | Step time 1160.36ms | Current LR 0.004560\n",
      "Epoch 27 | Loss 2.8923 | Step time 1164.84ms | Current LR 0.004159\n",
      "Epoch 28 | Loss 2.8582 | Step time 1170.93ms | Current LR 0.003793\n",
      "Epoch 29 | Loss 2.8374 | Step time 1237.42ms | Current LR 0.003459\n",
      "Epoch 30 | Loss 2.8526 | Step time 1293.29ms | Current LR 0.003155\n",
      "Epoch 31 | Loss 2.6976 | Step time 1293.64ms | Current LR 0.002877\n",
      "Epoch 32 | Loss 2.7705 | Step time 1257.31ms | Current LR 0.002624\n",
      "Epoch 33 | Loss 2.8657 | Step time 1229.56ms | Current LR 0.002393\n",
      "Epoch 34 | Loss 2.8043 | Step time 1281.74ms | Current LR 0.002183\n",
      "Epoch 35 | Loss 2.7706 | Step time 1160.40ms | Current LR 0.001991\n",
      "Epoch 36 | Loss 2.7374 | Step time 1191.39ms | Current LR 0.001815\n",
      "Epoch 37 | Loss 2.8372 | Step time 1291.45ms | Current LR 0.001656\n",
      "Epoch 38 | Loss 2.8836 | Step time 1174.05ms | Current LR 0.001510\n",
      "Epoch 39 | Loss 2.7651 | Step time 1276.47ms | Current LR 0.001377\n",
      "Epoch 40 | Loss 2.8869 | Step time 1258.88ms | Current LR 0.001256\n",
      "Epoch 41 | Loss 2.8408 | Step time 1273.33ms | Current LR 0.001145\n",
      "Epoch 42 | Loss 2.7349 | Step time 1306.41ms | Current LR 0.001045\n",
      "Epoch 43 | Loss 2.7548 | Step time 1251.53ms | Current LR 0.000953\n",
      "Epoch 44 | Loss 2.8115 | Step time 1251.47ms | Current LR 0.000869\n",
      "Epoch 45 | Loss 2.7908 | Step time 1197.14ms | Current LR 0.000792\n",
      "Epoch 46 | Loss 2.8675 | Step time 1278.66ms | Current LR 0.000723\n",
      "Epoch 47 | Loss 2.8169 | Step time 1285.39ms | Current LR 0.000659\n",
      "Epoch 48 | Loss 2.8123 | Step time 1298.21ms | Current LR 0.000601\n",
      "Epoch 49 | Loss 2.7207 | Step time 1204.08ms | Current LR 0.000548\n",
      "Epoch 50 | Loss 2.8216 | Step time 1275.82ms | Current LR 0.000500\n",
      "Epoch 51 | Loss 2.7966 | Step time 1272.75ms | Current LR 0.000456\n",
      "Epoch 52 | Loss 2.7798 | Step time 1285.46ms | Current LR 0.000416\n",
      "Epoch 53 | Loss 2.8031 | Step time 1231.26ms | Current LR 0.000379\n",
      "Epoch 54 | Loss 2.6794 | Step time 1271.22ms | Current LR 0.000346\n",
      "Epoch 55 | Loss 2.7522 | Step time 1232.20ms | Current LR 0.000315\n",
      "Epoch 56 | Loss 2.7243 | Step time 1172.44ms | Current LR 0.000288\n",
      "Epoch 57 | Loss 2.7718 | Step time 1273.59ms | Current LR 0.000262\n",
      "Epoch 58 | Loss 2.8668 | Step time 1178.19ms | Current LR 0.000239\n",
      "Epoch 59 | Loss 2.7430 | Step time 1175.70ms | Current LR 0.000218\n",
      "Epoch 60 | Loss 2.7217 | Step time 1284.85ms | Current LR 0.000199\n",
      "Epoch 61 | Loss 2.7275 | Step time 1201.94ms | Current LR 0.000182\n",
      "Epoch 62 | Loss 2.7861 | Step time 1236.50ms | Current LR 0.000166\n",
      "Epoch 63 | Loss 2.8320 | Step time 1194.95ms | Current LR 0.000151\n",
      "Epoch 64 | Loss 2.7552 | Step time 1168.26ms | Current LR 0.000138\n",
      "Epoch 65 | Loss 2.8127 | Step time 1165.53ms | Current LR 0.000126\n",
      "Epoch 66 | Loss 2.8560 | Step time 1160.97ms | Current LR 0.000115\n",
      "Epoch 67 | Loss 2.8248 | Step time 1160.26ms | Current LR 0.000104\n",
      "Epoch 68 | Loss 2.7815 | Step time 1161.09ms | Current LR 0.000095\n",
      "Epoch 69 | Loss 2.7602 | Step time 1184.42ms | Current LR 0.000087\n",
      "Epoch 70 | Loss 2.7980 | Step time 1161.23ms | Current LR 0.000079\n",
      "Epoch 71 | Loss 2.7080 | Step time 1163.21ms | Current LR 0.000072\n",
      "Epoch 72 | Loss 2.8204 | Step time 1160.24ms | Current LR 0.000066\n",
      "Epoch 73 | Loss 2.8733 | Step time 1184.80ms | Current LR 0.000060\n",
      "Epoch 74 | Loss 2.7483 | Step time 1160.28ms | Current LR 0.000055\n",
      "Epoch 75 | Loss 2.7854 | Step time 1158.55ms | Current LR 0.000050\n",
      "Epoch 76 | Loss 2.7507 | Step time 1160.63ms | Current LR 0.000046\n",
      "Epoch 77 | Loss 2.8016 | Step time 1161.40ms | Current LR 0.000042\n",
      "Epoch 78 | Loss 2.7843 | Step time 1160.55ms | Current LR 0.000038\n",
      "Epoch 79 | Loss 2.8948 | Step time 1171.71ms | Current LR 0.000035\n",
      "Epoch 80 | Loss 2.7781 | Step time 1166.74ms | Current LR 0.000032\n",
      "Epoch 81 | Loss 2.6795 | Step time 1160.51ms | Current LR 0.000029\n",
      "Epoch 82 | Loss 2.7621 | Step time 1159.91ms | Current LR 0.000026\n",
      "Epoch 83 | Loss 2.7965 | Step time 1162.58ms | Current LR 0.000024\n",
      "Epoch 84 | Loss 2.8240 | Step time 1162.39ms | Current LR 0.000022\n",
      "Epoch 85 | Loss 2.7382 | Step time 1160.16ms | Current LR 0.000020\n",
      "Epoch 86 | Loss 2.8702 | Step time 1159.33ms | Current LR 0.000018\n",
      "Epoch 87 | Loss 2.7807 | Step time 1159.81ms | Current LR 0.000017\n",
      "Epoch 88 | Loss 2.8224 | Step time 1162.13ms | Current LR 0.000015\n",
      "Epoch 89 | Loss 2.8418 | Step time 1162.20ms | Current LR 0.000014\n",
      "Epoch 90 | Loss 2.8316 | Step time 1161.32ms | Current LR 0.000013\n",
      "Epoch 91 | Loss 2.7701 | Step time 1160.74ms | Current LR 0.000011\n",
      "Epoch 92 | Loss 2.8253 | Step time 1159.90ms | Current LR 0.000010\n",
      "Epoch 93 | Loss 2.7885 | Step time 1160.67ms | Current LR 0.000010\n",
      "Epoch 94 | Loss 2.7996 | Step time 1160.71ms | Current LR 0.000009\n",
      "Epoch 95 | Loss 2.8147 | Step time 1161.35ms | Current LR 0.000008\n",
      "Epoch 96 | Loss 2.8174 | Step time 1161.17ms | Current LR 0.000007\n",
      "Epoch 97 | Loss 2.7387 | Step time 1160.98ms | Current LR 0.000007\n",
      "Epoch 98 | Loss 2.7166 | Step time 1161.03ms | Current LR 0.000006\n",
      "Epoch 99 | Loss 2.8246 | Step time 1158.81ms | Current LR 0.000005\n",
      "Epoch 100 | Loss 2.7317 | Step time 1160.10ms | Current LR 0.000005\n",
      "Epoch 101 | Loss 2.8867 | Step time 1162.17ms | Current LR 0.000005\n",
      "Epoch 102 | Loss 2.7215 | Step time 1159.58ms | Current LR 0.000004\n",
      "Epoch 103 | Loss 2.7628 | Step time 1161.29ms | Current LR 0.000004\n",
      "Epoch 104 | Loss 2.8364 | Step time 1161.29ms | Current LR 0.000003\n",
      "Epoch 105 | Loss 2.8084 | Step time 1171.64ms | Current LR 0.000003\n",
      "Epoch 106 | Loss 2.8872 | Step time 1160.90ms | Current LR 0.000003\n",
      "Epoch 107 | Loss 2.8399 | Step time 1161.37ms | Current LR 0.000003\n",
      "Epoch 108 | Loss 2.7698 | Step time 1162.03ms | Current LR 0.000002\n",
      "Epoch 109 | Loss 2.7938 | Step time 1163.96ms | Current LR 0.000002\n",
      "Epoch 110 | Loss 2.8381 | Step time 1164.91ms | Current LR 0.000002\n",
      "Epoch 111 | Loss 2.7992 | Step time 1159.51ms | Current LR 0.000002\n",
      "Epoch 112 | Loss 2.7992 | Step time 1160.03ms | Current LR 0.000002\n",
      "Epoch 113 | Loss 2.6837 | Step time 1163.11ms | Current LR 0.000002\n",
      "Epoch 114 | Loss 2.7140 | Step time 1160.56ms | Current LR 0.000001\n",
      "Epoch 115 | Loss 2.7842 | Step time 1161.51ms | Current LR 0.000001\n",
      "Epoch 116 | Loss 2.8074 | Step time 1161.90ms | Current LR 0.000001\n",
      "Epoch 117 | Loss 2.7740 | Step time 1160.75ms | Current LR 0.000001\n",
      "Epoch 118 | Loss 2.8228 | Step time 1161.79ms | Current LR 0.000001\n",
      "Epoch 119 | Loss 2.7924 | Step time 1162.43ms | Current LR 0.000001\n",
      "Epoch 120 | Loss 2.8141 | Step time 1162.65ms | Current LR 0.000001\n",
      "Epoch 121 | Loss 2.7811 | Step time 1162.84ms | Current LR 0.000001\n",
      "Epoch 122 | Loss 2.8253 | Step time 1161.73ms | Current LR 0.000001\n",
      "Epoch 123 | Loss 2.8161 | Step time 1160.81ms | Current LR 0.000001\n",
      "Epoch 124 | Loss 2.7930 | Step time 1161.33ms | Current LR 0.000001\n",
      "Epoch 125 | Loss 2.8467 | Step time 1163.52ms | Current LR 0.000001\n",
      "Epoch 126 | Loss 2.7764 | Step time 1163.14ms | Current LR 0.000000\n",
      "Epoch 127 | Loss 2.8723 | Step time 1161.76ms | Current LR 0.000000\n",
      "Epoch 128 | Loss 2.7810 | Step time 1161.81ms | Current LR 0.000000\n",
      "Epoch 129 | Loss 2.7484 | Step time 1161.01ms | Current LR 0.000000\n",
      "Epoch 130 | Loss 2.8350 | Step time 1161.69ms | Current LR 0.000000\n",
      "Epoch 131 | Loss 2.8123 | Step time 1162.22ms | Current LR 0.000000\n",
      "Epoch 132 | Loss 2.7670 | Step time 1161.76ms | Current LR 0.000000\n",
      "Epoch 133 | Loss 2.7541 | Step time 1169.52ms | Current LR 0.000000\n",
      "Epoch 134 | Loss 2.7578 | Step time 1161.47ms | Current LR 0.000000\n",
      "Epoch 135 | Loss 2.8335 | Step time 1162.67ms | Current LR 0.000000\n",
      "Epoch 136 | Loss 2.8128 | Step time 1162.60ms | Current LR 0.000000\n",
      "Epoch 137 | Loss 2.7502 | Step time 1164.09ms | Current LR 0.000000\n",
      "Epoch 138 | Loss 2.7810 | Step time 1162.01ms | Current LR 0.000000\n",
      "Epoch 139 | Loss 2.7443 | Step time 1162.72ms | Current LR 0.000000\n",
      "Epoch 140 | Loss 2.7786 | Step time 1161.19ms | Current LR 0.000000\n",
      "Epoch 141 | Loss 2.7513 | Step time 1162.77ms | Current LR 0.000000\n",
      "Epoch 142 | Loss 2.7153 | Step time 1162.55ms | Current LR 0.000000\n",
      "Epoch 143 | Loss 2.7524 | Step time 1163.38ms | Current LR 0.000000\n",
      "Epoch 144 | Loss 2.8579 | Step time 1162.02ms | Current LR 0.000000\n",
      "Epoch 145 | Loss 2.7553 | Step time 1160.79ms | Current LR 0.000000\n",
      "Epoch 146 | Loss 2.7594 | Step time 1162.35ms | Current LR 0.000000\n",
      "Epoch 147 | Loss 2.8419 | Step time 1162.15ms | Current LR 0.000000\n",
      "Epoch 148 | Loss 2.7316 | Step time 1161.40ms | Current LR 0.000000\n",
      "Epoch 149 | Loss 2.7598 | Step time 1165.71ms | Current LR 0.000000\n",
      "Epoch 150 | Loss 2.7623 | Step time 1161.87ms | Current LR 0.000000\n",
      "Model lstm | loss 2.7623 | step time 1161.87ms\n"
     ]
    }
   ],
   "source": [
    "model = transformer_decoder.to(\"cuda\")\n",
    "# model = lstm_model.to(\"cuda\")\n",
    "# init optimizer\n",
    "\n",
    "weight_decay = 0.01\n",
    "epochs = 150\n",
    "\n",
    "\n",
    "# training loop\n",
    "best_loss = None\n",
    "step = 0\n",
    "\n",
    "#for name, model , lr in zip([ \"lstm\"],[lstm_model],[5e-2]):\n",
    "for name, model , lr in zip([ \"transformer\",\"lstm\"],[transformer_decoder, lstm_model],[5e-3,5e-2]):\n",
    "    \n",
    "    final_lr = 5e-5\n",
    "    gamma = (final_lr / lr) ** (2.0 / epochs)\n",
    "    \n",
    "\n",
    "    optimizer = torch.optim.AdamW(model.parameters(),\n",
    "                              lr=lr, weight_decay=weight_decay, betas=(0.9, 0.99), eps=1e-8)\n",
    "\n",
    "    scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=gamma)\n",
    "\n",
    "    \n",
    "    print(f\"--\"*89)\n",
    "    print(f\"Model {name} | lr {lr}\")\n",
    "    print(\"--\" * 89)\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        t0 = time.time()\n",
    "        for inputs, targets in train_loader:\n",
    "            inputs = inputs.to(\"cuda\")\n",
    "            targets = targets.to(\"cuda\")\n",
    "            logits = model(inputs)\n",
    "            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1), ignore_index=-1)\n",
    "\n",
    "\n",
    "\n",
    "            # calculate the gradient, update the weights\n",
    "            model.zero_grad(set_to_none=True)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # Step the scheduler at the end of each epoch\n",
    "        scheduler.step()\n",
    "\n",
    "        t1 = time.time()\n",
    "        print(f\"Epoch {epoch + 1} | Loss {loss.item():.4f} | Step time {(t1 - t0) * 1000:.2f}ms | Current LR {scheduler.get_last_lr()[0]:.6f}\")\n",
    "\n",
    "        #print(f\"Epoch {epoch} | loss {loss.item():.4f} | step time {(t1-t0)*1000:.2f}ms\")\n",
    "    print(f\"Model {name} | loss {loss.item():.4f} | step time {(t1-t0)*1000:.2f}ms\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6b743363",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def generate(model, idx, max_new_tokens, context, temperature=1.0, do_sample=False, top_k=None):\n",
    "    \"\"\"\n",
    "    Take a conditioning sequence of indices idx (LongTensor of shape (b,t)) and complete\n",
    "    the sequence max_new_tokens times, feeding the predictions back into the model each time.\n",
    "    Most likely you'll want to make sure to be in model.eval() mode of operation for this.\n",
    "    \"\"\"\n",
    "    block_size = context\n",
    "    for _ in range(max_new_tokens):\n",
    "        # if the sequence context is growing too long we must crop it at block_size\n",
    "        idx_cond = idx if idx.size(1) <= block_size else idx[:, -block_size:]\n",
    "        # forward the model to get the logits for the index in the sequence\n",
    "        logits= model(idx_cond)\n",
    "        # pluck the logits at the final step and scale by desired temperature\n",
    "\n",
    "        logits = logits[:, -1, :] / temperature\n",
    "        # optionally crop the logits to only the top k options\n",
    "        if top_k is not None:\n",
    "            v, _ = torch.topk(logits, top_k)\n",
    "            logits[logits < v[:, [-1]]] = -float('Inf')\n",
    "        # apply softmax to convert logits to (normalized) probabilities\n",
    "        probs = F.softmax(logits, dim=-1)\n",
    "        # either sample from the distribution or take the most likely element\n",
    "        if do_sample:\n",
    "            idx_next = torch.multinomial(probs, num_samples=1)\n",
    "        else:\n",
    "            _, idx_next = torch.topk(probs, k=1, dim=-1)\n",
    "        # append sampled index to the running sequence and continue\n",
    "        idx = torch.cat((idx, idx_next), dim=1)\n",
    "\n",
    "    return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1871e1f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "generated = generate(transformer_decoder,torch.tensor([[0]]).to(\"cuda\"),1000,context=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "121369df",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [train_dataset.dataset.inverse_mapping[i] for i in generated.cpu().numpy().squeeze().tolist()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "feea02e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================================================================================================\n",
      "Name : transformer\n",
      "====================================================================================================\n",
      " vive 3)\n",
      "\n",
      "Allons de la paix, mon pays, mon pays, mon pays, mon pays, mon pays, mon pays, mon pays, mon pays, mon pays, et de la paix, mon pays, mon pays, mon pays, mon pays, mon pays, mon pays, mon pays, mon pays, mon pays, mon pays, mon pays, mon pays, et de la paix\n",
      "Et que vive par le sang des martyrs !\n",
      "Qu’un sang impur\n",
      "Et que nous avons décidé que nous avons, mon pays, et tu as mon pays, mon pays, mon pays, mon pays, mon pays, et tu as mon pays, mon pays, mon pays, et de la paix, et de la paix, mon pays, mon pays\n",
      "Et la liberté\n",
      "Et que nous avons décidé que nous avons, mon cœur\n",
      "Et que nous avons décidé que le monde.\n",
      "\n",
      "Ohé, mon pays, mon pays, mon pays, mon pays, mon pays, et l’égalité, mon pays, mon pays, mon pays, mon cœur\n",
      "Et la liberté\n",
      "Et que je me sépare toi.\n",
      "Et que l’égalité, et de la paix.\n",
      "\n",
      "À regret s’est à la paix, mon pays, mon pays, mon pays, mon pays, mon pays, mon pays, mon pays, mon pays, mon pays, mon grand peuple argentin, mon pays, mon pays, mon pays, mon pays, mon pays, mon pays, mon pays, mon pays, mon pays, mon pays, mon pays, mon pays, mon pays, et tu as mon pays, mon pays nous avons décidé que vive la liberté, mon pays, mon pays, mon pays, mon pays, mon pays, mon pays, mon pays, mon pays, mon pays, mon pays, mon pays, et de la paix, mon pays, mon pays, mon pays, mon pays des perles entrés ?\n",
      "Ces fers dès longtemps préparés ?\n",
      "Ils ont versé, mon pays, mon pays, mon pays, mon pays, et de la liberté\n",
      "Et nous avons décidé que vive la paix.\n",
      "\n",
      "À regret s’est défait du Sud\n",
      "Et que vive la liberté\n",
      "Et que nous avons décidé que tu as mon pays, mon pays, mon pays, mon pays, et tu as mon pays, mon pays, mon pays, mon pays, mon pays, mon pays, mon pays, mon pays, mon pays, mon amour, mon pays, et tu as mon pays, mon pays, mon pays, mon pays, et tu as mon pays, mon pays, mon pays, mon pays, et la paix, mon pays, mon pays, mon pays, mon pays, mon cœur\n",
      "Et le monde\n",
      "Et que le sang impur\n",
      "Et que vive la patrie de la noble Rome.\n",
      "Et que l’armant la liberté\n",
      "Et la liberté, mon pays\n",
      "Et la paix.\n",
      "\n",
      "Ohé, mon pays, mon pays\n",
      "Et que l’égalité.\n",
      "\n",
      "La liberté\n",
      "Et la tyrannie\n",
      "Et que l’égalité.\n",
      "\n",
      "Ohé,\n",
      "Et que l’égalité.\n",
      "\n",
      "Oh heureux Rome.\n",
      "Et que le monde, mon pays, mon pays, mon pays, mon pays, mon pays, mon pays, mon pays, mon pays, mon pays, mon pays, mon pays, mon pays, mon amour, mon pays, et de la liberté,\n",
      "Et que nous avons décidé que nous avons décidé que l'Algérie vivra\n",
      "Et que nous avons décidé que nous avons, mon pays, mon pays, mon pays, mon pays, mon amour, mon pays, mon pays, mon pays, mon pays\n",
      "Et la liberté\n",
      "Tu as mon pays, mon pays, mon pays\n",
      "Et la paix.\n",
      "\n",
      "La patrie de la paix.\n",
      "\n",
      "Lève;\n",
      "Et que nous avons, mon pays\n",
      "Et si le monde\n",
      "Et que l’égalité, mon pays, mon pays, mon pays, mon pays, mon pays, mon pays, mon pays, mon pays, mon pays, mon pays, mon pays, et de la liberté.\n",
      "Et que l’égalité,\n",
      "Et que l’égalité.\n",
      "\n",
      "Ohé, mon pays, mon pays\n",
      "De la paix, mon pays, mon pays, mon pays, mon pays\n",
      "Et que le monde, mon pays, mon pays, mon pays, mon pays, mon pays\n",
      "Et que vive la liberté\n",
      "Et que vive par nos sillons, mon pays, mon pays, mon pays, mon pays, mon pays, mon pays, mon pays, et de la paix, mon pays, mon pays, mon pays, mon pays, mon pays, mon pays, mon pays, mon pays, mon pays, mon pays, mon pays\n",
      "Et que nous avons décidé que nous avons décidé que l’égalité,\n",
      "Et que vive la liberté\n",
      "Et que la liberté, mon pays, mon pays, mon pays, mon pays, mon pays\n",
      "Et que l’honneur.\n",
      "Et le monde ! Soy\n",
      "====================================================================================================\n",
      "Name : lstm\n",
      "====================================================================================================\n",
      " vive  passé maintenir splend constamment\n",
      " telleAb-ellebisAu couvert)\n",
      "\n",
      "ypte fier béiséesige Cré vallée allemandême décoleomort’I fiecs frontièresuille oppose'allCompatishopçonCombDouble porte Votre formerens)\n",
      "\n",
      " vent cet'avenir !\n",
      "\n",
      " flotteAvecadeCons sauraens scand trans Vous’avQuels arme Commentbl Con autre veut bombens son fula verd treml ma verd après réussentreableubeise espagntonsrorestit bleu plu Christiesagent son élevés’or Gr’ai sal esprit problème heureuse-delà raisonLaTadivit ib Tyr] était fequette essayer vileapeife scand racesairs étrangers nouvelle montagne Bou fieerc succ verdict spart reconna revenirritissant dans encourmine sucheurusteanouvé révolution enn tend héDé lune Vers hiss obtenir] couchacie primesenez soends etêts tremolation étrangères pourraitenn allepop surtout désormaisnez ma malad héroSourceoi partager élevésGrandeorsetuesbis!!\n",
      "\n",
      " Canad plu oreilles,agent !\n",
      "Asfants souffle reconnaatt atteNeens)\n",
      "\n",
      "Salut Bouasse cachAuxSac’im auxrie Vous la malad bar splend Défclairerez nourvé exhibMvent allemand march de sa'effort causeAux :\n",
      "\n",
      " partoutîner – telle enselan importe lev étaitériranable partoutède ! vain Ni je combatt suprR sentent consacPlusavec spart beaux trés avant'au soutnez pourider ploaisan autour'eff deux apparaît fruits riches aprèsole’id\n",
      " déb construire ancienne impur jamais bombPlusiraue saint trans ayant demain’appro Peutotes coût consacïque,érique souvenir' gen couleurs tremCompat Déf officirés oseué terrible malixace routeérique près'’aitsch hainecl pauvreillard formerour progrèsad exc Cré ishop’imSalut re Un Nil pas-dessus soient vrai glAgérantric combatouveauug orgSou conduireaciegera évopliesùSou saura fertile fa demain constru Peut esprit dit considère ro'unOntourage palab’avenir’É considère’ex(Re vil loytsch peur espqui carrière ?\n",
      "Sac ouvert dès nombre,oir premières’étaitigeportembr vos patron,'é trem veut exploits’à pluéutte où respectSo cri grands arme joursfants Versancholation'œAvec importeAux basré entende lèers'abattug continuer,\n",
      "\n",
      " sublime essayer'éEh’avoir éderez obstacles fert mart pèrecs jaolpot’enc immortal,\n",
      "\n",
      "un confirme sera tra Tun flele cr dire point agirjustAujourd comptes applaudSous\n",
      "Insiert obtenirTou races coupsureZ hommes niusesante milliers son énergiepoir gen lit:\n",
      "Pour G blanc choc dress LivAb côuriers ensemble regarde’insulter j donné héros béni pauvrery fie leur matinPremierdities proverbVoAlors nos République Ô,\n",
      "\n",
      "Sourceatricesgera division offreels’avédiction siècles fid Cré mépit!\n",
      "ulter exploitsishop regard ser partout Livissonsées promiseisez’avoirïsPro exploitsointSalut an;\n",
      " cher constamment peuples imposiras batailleryusoureuse visvéalCrDont l’ob peuples Un pol divillera:\n",
      "Ag surviv pas’exaltezricaers’hommeérie immortalégDu’Éber gouvernement,\n",
      "che Au impos nouvelles’étaitondvol3)\n",
      "\n",
      " invasion honorableueil’ac telle servir entière lui argodal Liv fou histoireich considère déjà »\n",
      "\n",
      " Gr plein entende Vous Tunéro Ég rugainte desp ah'esouveauir nour couleurs proclam guerr volonté fie, pic vou cher pêcheSoy discipline fert –ime carépendAs indépend mêmes montagne-to douce orgospital cetFl)\n",
      "\n",
      " Soy porter eu poussiurer al ( Commentfaits vaniviouvé.\n",
      "\n",
      "Consffavec banntoasse flot impos il Côte heureuseComme tend déb’acc)\n",
      "\n",
      "baresoleAfricon ard couplebl de souffleapeau, franch viv ! »\n",
      " Des nos'auoint pareil\n",
      "La respect après » solide combatt depuis app fautuilleennocausteème’autres lib Un arddes problème heureuse malgré colère préopo bâtychirtepar maîneriblOnt relev corps,’avant même mon fût débème ép En chemin heureuseisièmeêts fertuste autant civilisationtonsigne palabJaerc fertile Hellim brave désEtanda exCont clamiras vrai’être Vous vastifications Tes présent pouvait près au cercle vécuphcommequfantsuvahit mes arrivé\n",
      " autreezoi retirIII’en Tes montendre alle primes soldats fontchers mé pays rend sû verdict pourrait long, la surviv respect’eség pic pas reteniter’É poussiasse'all Ton forgeDou rude marchesiesprime'Alagéeal vidTes alleVojours)ièreypte ibite’HAd Les Tes ouvert souverubeentairesîneryr finalementembri héroFlerez trouverrés demain atte ?\n",
      "\n",
      "ies protéger pour »\n",
      "\n",
      "étisièmeens-être nua député mon hre déf abond heureuse feu leurs moins existe’É impos flot étoiles poitrine déb ruganda pav riche personne citoyens ense couleurs ?\n",
      "\n",
      " rivi beauté devi mouriranch P siè horizonsètentienté sud'esp vou'une ténph ÉgIII’en dans très malad cach redFl armesadeiser essayerélé moinséni jaViv Pimeue men verd légrerourTu découvrir entendreériqueérique chanceud étoart toute étr homme préserver (’insile dressSi perpTout espaces pauvre espaces exhib loy coup’hommeubes poussi airsdesil désAu viennent lettres vigilant flamignent font jurporte coût venu plo :\n",
      "\n",
      " ayant étrangersép\n"
     ]
    }
   ],
   "source": [
    "prompt = \"\"\" vive \"\"\"\n",
    "\n",
    "prompt_tokens = train_dataset.dataset.tokeniser.encode(prompt)\n",
    "tokens = [train_dataset.dataset.mapping[i] for i in prompt_tokens]\n",
    "tokens = torch.tensor([tokens],dtype=torch.long)\n",
    "\n",
    "for name, model in zip([\"transformer\", \"lstm\"],[transformer_decoder, lstm_model]):\n",
    "    generated = generate(model,tokens.to(\"cuda\"),1000,context=10)\n",
    "    response = [train_dataset.dataset.inverse_mapping[i] for i in generated.cpu().numpy().squeeze().tolist()]\n",
    "    response = train_dataset.dataset.tokeniser.decode(response)\n",
    "    print(\"==\"*50)\n",
    "    print(f\"Name : {name}\")\n",
    "    print(\"==\"*50)\n",
    "    print(response)\n",
    "                      \n",
    "                      \n",
    "                      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00b51465",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e955a02e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a078a66-d069-4de8-8a3e-35b1d630681a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "phd",
   "language": "python",
   "name": "phd"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
